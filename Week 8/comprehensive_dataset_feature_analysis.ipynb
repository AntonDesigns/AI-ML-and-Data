{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔍 Comprehensive Feature Analysis & Dataset Combination\n",
    "## Analyzing ALL Fontys Travel Datasets\n",
    "\n",
    "**Goal:** \n",
    "1. Analyze features in ALL 6 datasets\n",
    "2. Compare which dataset has the BEST features\n",
    "3. Identify which datasets can be JOINED together\n",
    "4. Create a COMBINED dataset for modeling\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"✅ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 📂 Section 1: Load ALL Datasets\n",
    "\n",
    "Let's load all 6 datasets and handle any loading issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "\n",
      "================================================================================\n",
      "❌ df1_trip_flight: ERROR - [Errno 2] No such file or directory: 'trip-overview-flight-20251013010007-fontys.csv'\n",
      "❌ df2_deelnemer: ERROR - [Errno 2] No such file or directory: 'export_Deelnemer_boekingen_Vertrekkend_januari_2025-fontys.xls'\n",
      "❌ df3_reizen_riss: ERROR - [Errno 2] No such file or directory: 'export_reizen_Riss_31-12-2025-fontys.xlsx'\n",
      "❌ df4_overzicht_cax: ERROR - [Errno 2] No such file or directory: 'Overzicht_reizen_vanuit_cax-GV_vertrek_juni_2025_en_verder_-_fontys.xlsx'\n",
      "❌ df5_boekingen_total: ERROR - [Errno 2] No such file or directory: 'Export_boekingen_total_RISS_31-12-2024_-_fontys.xlsx'\n",
      "❌ df6_grip_freeze: ERROR - [Errno 2] No such file or directory: 'Grip_export_booking_freeze_1_wk-fontys.xlsx'\n",
      "================================================================================\n",
      "\n",
      "✅ Successfully loaded 0 out of 6 datasets!\n"
     ]
    }
   ],
   "source": [
    "# Define file paths\n",
    "files = {\n",
    "    'df1_trip_flight': 'trip-overview-flight-20251013010007-fontys.csv',\n",
    "    'df2_deelnemer': 'export_Deelnemer_boekingen_Vertrekkend_januari_2025-fontys.xls',\n",
    "    'df3_reizen_riss': 'export_reizen_Riss_31-12-2025-fontys.xlsx',\n",
    "    'df4_overzicht_cax': 'Overzicht_reizen_vanuit_cax-GV_vertrek_juni_2025_en_verder_-_fontys.xlsx',\n",
    "    'df5_boekingen_total': 'Export_boekingen_total_RISS_31-12-2024_-_fontys.xlsx',\n",
    "    'df6_grip_freeze': 'Grip_export_booking_freeze_1_wk-fontys.xlsx'\n",
    "}\n",
    "\n",
    "datasets = {}\n",
    "\n",
    "print(\"Loading datasets...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, file in files.items():\n",
    "    try:\n",
    "        if file.endswith('.csv'):\n",
    "            # Try different encodings and separators for CSV\n",
    "            try:\n",
    "                df = pd.read_csv(file, encoding='utf-8')\n",
    "            except:\n",
    "                try:\n",
    "                    df = pd.read_csv(file, encoding='latin-1')\n",
    "                except:\n",
    "                    df = pd.read_csv(file, encoding='utf-8', on_bad_lines='skip')\n",
    "        else:\n",
    "            df = pd.read_excel(file)\n",
    "        \n",
    "        datasets[name] = df\n",
    "        print(f\"✅ {name}: {len(df):,} rows × {len(df.columns)} columns\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ {name}: ERROR - {e}\")\n",
    "        datasets[name] = None\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n✅ Successfully loaded {sum(1 for v in datasets.values() if v is not None)} out of {len(files)} datasets!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 📊 Section 2: Quick Dataset Overview\n",
    "\n",
    "Let's see the basic structure of each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "DATASET OVERVIEW\n",
      "====================================================================================================\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "overview_data = []\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    if df is not None:\n",
    "        overview_data.append({\n",
    "            'Dataset': name,\n",
    "            'Rows': f\"{len(df):,}\",\n",
    "            'Columns': len(df.columns),\n",
    "            'Memory': f\"{df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\"\n",
    "        })\n",
    "\n",
    "overview_df = pd.DataFrame(overview_data)\n",
    "print(overview_df.to_string(index=False))\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 🔍 Section 3: Detailed Feature Analysis - Each Dataset\n",
    "\n",
    "Let's analyze the features (columns) in EACH dataset individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "📊 FEATURE ANALYSIS: df1_trip_flight\n",
      "====================================================================================================\n",
      "❌ Dataset not loaded\n",
      "\n",
      "====================================================================================================\n",
      "📊 FEATURE ANALYSIS: df2_deelnemer\n",
      "====================================================================================================\n",
      "❌ Dataset not loaded\n",
      "\n",
      "====================================================================================================\n",
      "📊 FEATURE ANALYSIS: df3_reizen_riss\n",
      "====================================================================================================\n",
      "❌ Dataset not loaded\n",
      "\n",
      "====================================================================================================\n",
      "📊 FEATURE ANALYSIS: df4_overzicht_cax\n",
      "====================================================================================================\n",
      "❌ Dataset not loaded\n",
      "\n",
      "====================================================================================================\n",
      "📊 FEATURE ANALYSIS: df5_boekingen_total\n",
      "====================================================================================================\n",
      "❌ Dataset not loaded\n",
      "\n",
      "====================================================================================================\n",
      "📊 FEATURE ANALYSIS: df6_grip_freeze\n",
      "====================================================================================================\n",
      "❌ Dataset not loaded\n"
     ]
    }
   ],
   "source": [
    "def analyze_dataset_features(df, dataset_name):\n",
    "    \"\"\"\n",
    "    Comprehensive feature analysis for a single dataset\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(f\"📊 FEATURE ANALYSIS: {dataset_name}\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    if df is None:\n",
    "        print(\"❌ Dataset not loaded\")\n",
    "        return None\n",
    "    \n",
    "    # Basic info\n",
    "    print(f\"\\n📏 Dimensions: {len(df):,} rows × {len(df.columns)} columns\")\n",
    "    print(f\"\\n📋 Column Names:\\n{', '.join(df.columns.tolist())}\")\n",
    "    \n",
    "    # Feature quality analysis\n",
    "    feature_quality = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        total = len(df)\n",
    "        missing = df[col].isna().sum()\n",
    "        missing_pct = (missing / total) * 100\n",
    "        unique = df[col].nunique()\n",
    "        unique_pct = (unique / total) * 100\n",
    "        dtype = str(df[col].dtype)\n",
    "        \n",
    "        # Quality score (0-100)\n",
    "        completeness_score = 100 - missing_pct\n",
    "        variety_score = min(unique_pct, 100) if unique > 1 else 0\n",
    "        quality_score = (completeness_score * 0.7) + (variety_score * 0.3)\n",
    "        \n",
    "        feature_quality.append({\n",
    "            'Column': col,\n",
    "            'Type': dtype,\n",
    "            'Missing': missing,\n",
    "            'Missing %': f\"{missing_pct:.1f}%\",\n",
    "            'Unique': unique,\n",
    "            'Unique %': f\"{unique_pct:.1f}%\",\n",
    "            'Quality Score': f\"{quality_score:.1f}\"\n",
    "        })\n",
    "    \n",
    "    feature_df = pd.DataFrame(feature_quality)\n",
    "    feature_df = feature_df.sort_values('Quality Score', ascending=False)\n",
    "    \n",
    "    print(f\"\\n🎯 FEATURE QUALITY RANKING:\\n\")\n",
    "    print(feature_df.to_string(index=False))\n",
    "    \n",
    "    # Summary statistics\n",
    "    avg_quality = feature_df['Quality Score'].str.replace('%', '').astype(float).mean()\n",
    "    print(f\"\\n📈 Average Feature Quality: {avg_quality:.1f}/100\")\n",
    "    \n",
    "    # First few rows\n",
    "    print(f\"\\n👀 Sample Data (first 3 rows):\\n\")\n",
    "    print(df.head(3).to_string())\n",
    "    \n",
    "    return feature_df\n",
    "\n",
    "# Analyze all datasets\n",
    "feature_analyses = {}\n",
    "for name, df in datasets.items():\n",
    "    feature_analyses[name] = analyze_dataset_features(df, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 🏆 Section 4: Compare Datasets - Which Has the BEST Features?\n",
    "\n",
    "Let's compare the overall quality across all datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "🏆 DATASET COMPARISON - WHICH HAS THE BEST FEATURES?\n",
      "====================================================================================================\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Avg Quality'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_25184\\3720993178.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     33\u001b[39m             \u001b[33m'DateTime'\u001b[39m: datetime_cols\n\u001b[32m     34\u001b[39m         })\n\u001b[32m     35\u001b[39m \n\u001b[32m     36\u001b[39m comparison_df = pd.DataFrame(comparison_data)\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m comparison_df = comparison_df.sort_values(\u001b[33m'Avg Quality'\u001b[39m, ascending=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     38\u001b[39m \n\u001b[32m     39\u001b[39m print(\u001b[33m\"\\n\"\u001b[39m + comparison_df.to_string(index=\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[32m     40\u001b[39m print(\u001b[33m\"\\n\"\u001b[39m + \u001b[33m\"=\"\u001b[39m*\u001b[32m100\u001b[39m)\n",
      "\u001b[32mc:\\Users\\Anton\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[39m\n\u001b[32m   7192\u001b[39m             )\n\u001b[32m   7193\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m len(by):\n\u001b[32m   7194\u001b[39m             \u001b[38;5;66;03m# len(by) == 1\u001b[39;00m\n\u001b[32m   7195\u001b[39m \n\u001b[32m-> \u001b[39m\u001b[32m7196\u001b[39m             k = self._get_label_or_level_values(by[\u001b[32m0\u001b[39m], axis=axis)\n\u001b[32m   7197\u001b[39m \n\u001b[32m   7198\u001b[39m             \u001b[38;5;66;03m# need to rewrap column in Series to apply key function\u001b[39;00m\n\u001b[32m   7199\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32mc:\\Users\\Anton\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1907\u001b[39m             values = self.xs(key, axis=other_axes[\u001b[32m0\u001b[39m])._values\n\u001b[32m   1908\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m self._is_level_reference(key, axis=axis):\n\u001b[32m   1909\u001b[39m             values = self.axes[axis].get_level_values(key)._values\n\u001b[32m   1910\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1911\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m KeyError(key)\n\u001b[32m   1912\u001b[39m \n\u001b[32m   1913\u001b[39m         \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[32m   1914\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m values.ndim > \u001b[32m1\u001b[39m:\n",
      "\u001b[31mKeyError\u001b[39m: 'Avg Quality'"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"🏆 DATASET COMPARISON - WHICH HAS THE BEST FEATURES?\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "comparison_data = []\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    if df is not None:\n",
    "        # Calculate metrics\n",
    "        total_cells = df.shape[0] * df.shape[1]\n",
    "        missing_cells = df.isna().sum().sum()\n",
    "        completeness = ((total_cells - missing_cells) / total_cells) * 100\n",
    "        \n",
    "        # Get average quality from feature analysis\n",
    "        if feature_analyses[name] is not None:\n",
    "            avg_quality = float(feature_analyses[name]['Quality Score'].str.replace('%', '').mean())\n",
    "        else:\n",
    "            avg_quality = 0\n",
    "        \n",
    "        # Calculate feature diversity (how many different types of features)\n",
    "        numeric_cols = len(df.select_dtypes(include=[np.number]).columns)\n",
    "        categorical_cols = len(df.select_dtypes(include=['object']).columns)\n",
    "        datetime_cols = len(df.select_dtypes(include=['datetime64']).columns)\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'Dataset': name,\n",
    "            'Rows': len(df),\n",
    "            'Features': len(df.columns),\n",
    "            'Completeness %': f\"{completeness:.1f}%\",\n",
    "            'Avg Quality': f\"{avg_quality:.1f}\",\n",
    "            'Numeric': numeric_cols,\n",
    "            'Categorical': categorical_cols,\n",
    "            'DateTime': datetime_cols\n",
    "        })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('Avg Quality', ascending=False)\n",
    "\n",
    "print(\"\\n\" + comparison_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Average Quality Score\n",
    "quality_scores = comparison_df['Avg Quality'].str.replace('%', '').astype(float)\n",
    "colors = ['#2ecc71' if x > 80 else '#f39c12' if x > 60 else '#e74c3c' for x in quality_scores]\n",
    "axes[0].barh(comparison_df['Dataset'], quality_scores, color=colors)\n",
    "axes[0].set_xlabel('Average Feature Quality Score', fontsize=12)\n",
    "axes[0].set_title('🏆 Dataset Quality Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].axvline(x=80, color='green', linestyle='--', alpha=0.5, label='Excellent (80+)')\n",
    "axes[0].axvline(x=60, color='orange', linestyle='--', alpha=0.5, label='Good (60+)')\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot 2: Feature Count and Types\n",
    "comparison_df.plot(x='Dataset', y=['Numeric', 'Categorical', 'DateTime'], \n",
    "                   kind='bar', stacked=True, ax=axes[1], \n",
    "                   color=['#3498db', '#e74c3c', '#f39c12'])\n",
    "axes[1].set_title('📊 Feature Type Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Number of Features')\n",
    "axes[1].set_xlabel('')\n",
    "axes[1].legend(title='Feature Type')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Winner announcement\n",
    "best_dataset = comparison_df.iloc[0]['Dataset']\n",
    "best_score = comparison_df.iloc[0]['Avg Quality']\n",
    "print(f\"\\n🥇 WINNER: {best_dataset} with average quality score of {best_score}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 🔗 Section 5: Identify JOIN KEYS - Which Datasets Can Work Together?\n",
    "\n",
    "Let's find common columns that can be used to merge datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"🔗 IDENTIFYING JOIN KEYS - Which Datasets Can Be Combined?\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Get all column names from all datasets\n",
    "all_columns = {}\n",
    "for name, df in datasets.items():\n",
    "    if df is not None:\n",
    "        all_columns[name] = set(df.columns)\n",
    "\n",
    "# Find common columns between datasets\n",
    "print(\"\\n📋 COMMON COLUMNS (Potential Join Keys):\\n\")\n",
    "\n",
    "dataset_names = list(all_columns.keys())\n",
    "join_possibilities = []\n",
    "\n",
    "for i, ds1 in enumerate(dataset_names):\n",
    "    for ds2 in dataset_names[i+1:]:\n",
    "        common = all_columns[ds1] & all_columns[ds2]\n",
    "        if common:\n",
    "            print(f\"\\n{ds1} ⟷ {ds2}\")\n",
    "            print(f\"  Common columns: {', '.join(sorted(common))}\")\n",
    "            join_possibilities.append({\n",
    "                'Dataset 1': ds1,\n",
    "                'Dataset 2': ds2,\n",
    "                'Common Columns': ', '.join(sorted(common)),\n",
    "                'Join Keys': len(common)\n",
    "            })\n",
    "\n",
    "if join_possibilities:\n",
    "    join_df = pd.DataFrame(join_possibilities)\n",
    "    join_df = join_df.sort_values('Join Keys', ascending=False)\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"\\n🎯 JOIN POSSIBILITIES RANKED:\\n\")\n",
    "    print(join_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"\\n⚠️ No common columns found between datasets!\")\n",
    "    print(\"   → Datasets may need to be analyzed separately\")\n",
    "    print(\"   → Or manual mapping/ID creation may be required\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 🧩 Section 6: Analyze Specific Join Keys\n",
    "\n",
    "Let's examine the quality of potential join keys in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_join_key(df1, df2, df1_name, df2_name, key_column):\n",
    "    \"\"\"\n",
    "    Analyze a potential join key between two datasets\n",
    "    \"\"\"\n",
    "    print(f\"\\n🔍 Analyzing JOIN KEY: '{key_column}'\")\n",
    "    print(f\"   Between: {df1_name} ⟷ {df2_name}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Get unique values\n",
    "    df1_values = set(df1[key_column].dropna().unique())\n",
    "    df2_values = set(df2[key_column].dropna().unique())\n",
    "    \n",
    "    # Calculate overlap\n",
    "    common_values = df1_values & df2_values\n",
    "    only_in_df1 = df1_values - df2_values\n",
    "    only_in_df2 = df2_values - df1_values\n",
    "    \n",
    "    # Calculate match percentage\n",
    "    match_pct = (len(common_values) / max(len(df1_values), len(df2_values))) * 100\n",
    "    \n",
    "    print(f\"\\n📊 Statistics:\")\n",
    "    print(f\"   {df1_name}: {len(df1_values):,} unique values\")\n",
    "    print(f\"   {df2_name}: {len(df2_values):,} unique values\")\n",
    "    print(f\"   Common values: {len(common_values):,}\")\n",
    "    print(f\"   Only in {df1_name}: {len(only_in_df1):,}\")\n",
    "    print(f\"   Only in {df2_name}: {len(only_in_df2):,}\")\n",
    "    print(f\"\\n✨ Match percentage: {match_pct:.1f}%\")\n",
    "    \n",
    "    # Determine if it's a good join key\n",
    "    if match_pct > 80:\n",
    "        quality = \"🟢 EXCELLENT - Strong overlap, great for joining!\"\n",
    "    elif match_pct > 50:\n",
    "        quality = \"🟡 GOOD - Moderate overlap, usable for joining\"\n",
    "    elif match_pct > 20:\n",
    "        quality = \"🟠 FAIR - Limited overlap, may lose data\"\n",
    "    else:\n",
    "        quality = \"🔴 POOR - Very little overlap, not recommended\"\n",
    "    \n",
    "    print(f\"\\n{quality}\")\n",
    "    \n",
    "    return {\n",
    "        'key': key_column,\n",
    "        'match_pct': match_pct,\n",
    "        'common': len(common_values)\n",
    "    }\n",
    "\n",
    "# Analyze each potential join\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"🔍 DETAILED JOIN KEY ANALYSIS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "join_quality = []\n",
    "\n",
    "for i, ds1_name in enumerate(dataset_names):\n",
    "    df1 = datasets[ds1_name]\n",
    "    if df1 is None:\n",
    "        continue\n",
    "    \n",
    "    for ds2_name in dataset_names[i+1:]:\n",
    "        df2 = datasets[ds2_name]\n",
    "        if df2 is None:\n",
    "            continue\n",
    "        \n",
    "        common_cols = all_columns[ds1_name] & all_columns[ds2_name]\n",
    "        \n",
    "        if common_cols:\n",
    "            for col in sorted(common_cols):\n",
    "                try:\n",
    "                    result = analyze_join_key(df1, df2, ds1_name, ds2_name, col)\n",
    "                    join_quality.append({\n",
    "                        'Dataset 1': ds1_name,\n",
    "                        'Dataset 2': ds2_name,\n",
    "                        'Join Key': col,\n",
    "                        'Match %': f\"{result['match_pct']:.1f}%\",\n",
    "                        'Common Values': result['common']\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"   ⚠️ Could not analyze column '{col}': {e}\")\n",
    "\n",
    "if join_quality:\n",
    "    join_quality_df = pd.DataFrame(join_quality)\n",
    "    join_quality_df['Match %'] = join_quality_df['Match %'].str.replace('%', '').astype(float)\n",
    "    join_quality_df = join_quality_df.sort_values('Match %', ascending=False)\n",
    "    join_quality_df['Match %'] = join_quality_df['Match %'].apply(lambda x: f\"{x:.1f}%\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"\\n🎯 BEST JOIN KEYS SUMMARY:\\n\")\n",
    "    print(join_quality_df.to_string(index=False))\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "\n",
    "print(\"\\n💡 TIP: Focus on join keys with >80% match for best results!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 🎯 Section 7: CREATE THE COMBINED DATASET\n",
    "\n",
    "Based on our analysis, let's create the best combined dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"🎯 CREATING COMBINED DATASET\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Strategy: Start with the highest quality dataset and merge others\n",
    "# Use the best join keys we identified\n",
    "\n",
    "# Get the best dataset (highest quality)\n",
    "best_dataset_name = comparison_df.iloc[0]['Dataset']\n",
    "combined_df = datasets[best_dataset_name].copy()\n",
    "\n",
    "print(f\"\\n🏁 Starting with: {best_dataset_name}\")\n",
    "print(f\"   Initial size: {len(combined_df):,} rows × {len(combined_df.columns)} columns\")\n",
    "\n",
    "# Try to merge other datasets\n",
    "merged_count = 0\n",
    "\n",
    "for ds_name in dataset_names:\n",
    "    if ds_name == best_dataset_name or datasets[ds_name] is None:\n",
    "        continue\n",
    "    \n",
    "    # Find common columns\n",
    "    common_cols = all_columns[best_dataset_name] & all_columns[ds_name]\n",
    "    \n",
    "    if not common_cols:\n",
    "        print(f\"\\n⚠️ Skipping {ds_name}: No common columns for joining\")\n",
    "        continue\n",
    "    \n",
    "    # Try each common column as join key\n",
    "    best_join_key = None\n",
    "    best_match = 0\n",
    "    \n",
    "    for col in common_cols:\n",
    "        try:\n",
    "            # Calculate match percentage\n",
    "            base_values = set(combined_df[col].dropna().unique())\n",
    "            new_values = set(datasets[ds_name][col].dropna().unique())\n",
    "            common_values = base_values & new_values\n",
    "            match_pct = (len(common_values) / max(len(base_values), len(new_values))) * 100\n",
    "            \n",
    "            if match_pct > best_match:\n",
    "                best_match = match_pct\n",
    "                best_join_key = col\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if best_join_key and best_match > 20:  # Only merge if >20% match\n",
    "        print(f\"\\n✅ Merging {ds_name} using '{best_join_key}' ({best_match:.1f}% match)\")\n",
    "        \n",
    "        # Perform the merge\n",
    "        before_cols = len(combined_df.columns)\n",
    "        combined_df = combined_df.merge(\n",
    "            datasets[ds_name],\n",
    "            on=best_join_key,\n",
    "            how='left',\n",
    "            suffixes=('', f'_{ds_name}')\n",
    "        )\n",
    "        after_cols = len(combined_df.columns)\n",
    "        new_cols = after_cols - before_cols\n",
    "        \n",
    "        print(f\"   Added {new_cols} new columns\")\n",
    "        merged_count += 1\n",
    "    else:\n",
    "        print(f\"\\n⚠️ Skipping {ds_name}: Poor match quality ({best_match:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(f\"\\n🎉 COMBINED DATASET CREATED!\")\n",
    "print(f\"   Final size: {len(combined_df):,} rows × {len(combined_df.columns)} columns\")\n",
    "print(f\"   Datasets merged: {merged_count + 1}\")\n",
    "print(\"\\n\" + \"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 📊 Section 8: Analyze the Combined Dataset\n",
    "\n",
    "Let's see what we have in our final combined dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"📊 COMBINED DATASET ANALYSIS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(f\"\\n📏 Dimensions: {len(combined_df):,} rows × {len(combined_df.columns)} columns\")\n",
    "print(f\"\\n📋 All Columns ({len(combined_df.columns)} total):\\n\")\n",
    "\n",
    "# Group columns by type\n",
    "numeric_cols = combined_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = combined_df.select_dtypes(include=['object']).columns.tolist()\n",
    "datetime_cols = combined_df.select_dtypes(include=['datetime64']).columns.tolist()\n",
    "\n",
    "print(f\"🔢 Numeric features ({len(numeric_cols)}):\")\n",
    "for col in numeric_cols[:20]:  # Show first 20\n",
    "    print(f\"   - {col}\")\n",
    "if len(numeric_cols) > 20:\n",
    "    print(f\"   ... and {len(numeric_cols) - 20} more\")\n",
    "\n",
    "print(f\"\\n📝 Categorical features ({len(categorical_cols)}):\")\n",
    "for col in categorical_cols[:20]:  # Show first 20\n",
    "    print(f\"   - {col}\")\n",
    "if len(categorical_cols) > 20:\n",
    "    print(f\"   ... and {len(categorical_cols) - 20} more\")\n",
    "\n",
    "if datetime_cols:\n",
    "    print(f\"\\n📅 DateTime features ({len(datetime_cols)}):\")\n",
    "    for col in datetime_cols:\n",
    "        print(f\"   - {col}\")\n",
    "\n",
    "# Data quality\n",
    "total_cells = combined_df.shape[0] * combined_df.shape[1]\n",
    "missing_cells = combined_df.isna().sum().sum()\n",
    "completeness = ((total_cells - missing_cells) / total_cells) * 100\n",
    "\n",
    "print(f\"\\n📈 Data Quality:\")\n",
    "print(f\"   Completeness: {completeness:.1f}%\")\n",
    "print(f\"   Missing values: {missing_cells:,} out of {total_cells:,} cells\")\n",
    "print(f\"   Memory usage: {combined_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Show sample\n",
    "print(f\"\\n👀 Sample Data (first 5 rows):\\n\")\n",
    "print(combined_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 🎯 Section 9: Feature Selection for Modeling\n",
    "\n",
    "Now let's identify which features are BEST for machine learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"🎯 FEATURE SELECTION FOR MODELING\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Analyze each feature for modeling potential\n",
    "feature_scores = []\n",
    "\n",
    "for col in combined_df.columns:\n",
    "    # Calculate metrics\n",
    "    missing_pct = (combined_df[col].isna().sum() / len(combined_df)) * 100\n",
    "    unique_count = combined_df[col].nunique()\n",
    "    unique_pct = (unique_count / len(combined_df)) * 100\n",
    "    dtype = str(combined_df[col].dtype)\n",
    "    \n",
    "    # Feature quality score (0-100)\n",
    "    # Factors:\n",
    "    # - Low missing values (70% weight)\n",
    "    # - Good variety but not too unique (30% weight)\n",
    "    completeness_score = 100 - missing_pct\n",
    "    \n",
    "    # Variety score: prefer 2-50% unique (not constant, not unique IDs)\n",
    "    if unique_count <= 1:\n",
    "        variety_score = 0  # Constant column\n",
    "    elif unique_pct > 95:\n",
    "        variety_score = 30  # Likely an ID, less useful\n",
    "    elif 2 <= unique_pct <= 50:\n",
    "        variety_score = 100  # Sweet spot!\n",
    "    else:\n",
    "        variety_score = 70\n",
    "    \n",
    "    total_score = (completeness_score * 0.7) + (variety_score * 0.3)\n",
    "    \n",
    "    # Determine recommendation\n",
    "    if total_score >= 80 and missing_pct < 30:\n",
    "        recommendation = \"✅ EXCELLENT - Highly recommended\"\n",
    "    elif total_score >= 60 and missing_pct < 50:\n",
    "        recommendation = \"🟢 GOOD - Recommended\"\n",
    "    elif total_score >= 40:\n",
    "        recommendation = \"🟡 FAIR - Consider with caution\"\n",
    "    else:\n",
    "        recommendation = \"❌ POOR - Not recommended\"\n",
    "    \n",
    "    feature_scores.append({\n",
    "        'Feature': col,\n",
    "        'Type': dtype,\n",
    "        'Missing %': f\"{missing_pct:.1f}%\",\n",
    "        'Unique': unique_count,\n",
    "        'Unique %': f\"{unique_pct:.1f}%\",\n",
    "        'Score': f\"{total_score:.1f}\",\n",
    "        'Recommendation': recommendation\n",
    "    })\n",
    "\n",
    "# Create DataFrame and sort\n",
    "feature_scores_df = pd.DataFrame(feature_scores)\n",
    "feature_scores_df['Score_Numeric'] = feature_scores_df['Score'].astype(float)\n",
    "feature_scores_df = feature_scores_df.sort_values('Score_Numeric', ascending=False)\n",
    "feature_scores_df = feature_scores_df.drop('Score_Numeric', axis=1)\n",
    "\n",
    "print(\"\\n🏆 FEATURE RANKING FOR MODELING:\\n\")\n",
    "print(feature_scores_df.to_string(index=False))\n",
    "\n",
    "# Summary statistics\n",
    "excellent = len(feature_scores_df[feature_scores_df['Recommendation'].str.contains('EXCELLENT')])\n",
    "good = len(feature_scores_df[feature_scores_df['Recommendation'].str.contains('GOOD')])\n",
    "fair = len(feature_scores_df[feature_scores_df['Recommendation'].str.contains('FAIR')])\n",
    "poor = len(feature_scores_df[feature_scores_df['Recommendation'].str.contains('POOR')])\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"\\n📊 FEATURE QUALITY SUMMARY:\")\n",
    "print(f\"   ✅ Excellent features: {excellent}\")\n",
    "print(f\"   🟢 Good features: {good}\")\n",
    "print(f\"   🟡 Fair features: {fair}\")\n",
    "print(f\"   ❌ Poor features: {poor}\")\n",
    "print(f\"\\n   💡 Recommended for modeling: {excellent + good} features\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Visualize feature quality distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Feature scores distribution\n",
    "scores = feature_scores_df['Score'].astype(float)\n",
    "axes[0].hist(scores, bins=20, color='#3498db', edgecolor='black')\n",
    "axes[0].axvline(x=80, color='green', linestyle='--', label='Excellent (80+)', linewidth=2)\n",
    "axes[0].axvline(x=60, color='orange', linestyle='--', label='Good (60+)', linewidth=2)\n",
    "axes[0].axvline(x=40, color='red', linestyle='--', label='Poor (<40)', linewidth=2)\n",
    "axes[0].set_xlabel('Feature Quality Score', fontsize=12)\n",
    "axes[0].set_ylabel('Number of Features', fontsize=12)\n",
    "axes[0].set_title('📊 Feature Quality Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot 2: Recommendation breakdown\n",
    "recommendation_counts = {\n",
    "    'Excellent': excellent,\n",
    "    'Good': good,\n",
    "    'Fair': fair,\n",
    "    'Poor': poor\n",
    "}\n",
    "colors = ['#2ecc71', '#3498db', '#f39c12', '#e74c3c']\n",
    "axes[1].pie(recommendation_counts.values(), labels=recommendation_counts.keys(), \n",
    "            autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "axes[1].set_title('🎯 Feature Recommendation Breakdown', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ✅ Section 10: Final Recommendations\n",
    "\n",
    "Let's create a summary of the best features to use for modeling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"✅ FINAL RECOMMENDATIONS FOR MODELING\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Get recommended features\n",
    "recommended_features = feature_scores_df[\n",
    "    feature_scores_df['Recommendation'].str.contains('EXCELLENT|GOOD', regex=True)\n",
    "]\n",
    "\n",
    "print(f\"\\n🎯 RECOMMENDED FEATURES ({len(recommended_features)} total):\\n\")\n",
    "print(recommended_features[['Feature', 'Type', 'Missing %', 'Score', 'Recommendation']].to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"\\n💡 NEXT STEPS FOR YOUR MODEL:\")\n",
    "print(\"=\"*100)\n",
    "print(\"\"\"\n",
    "1️⃣ **Start with EXCELLENT features** (highest quality)\n",
    "   → These have <20% missing values and good variety\n",
    "   → Perfect for initial model training\n",
    "\n",
    "2️⃣ **Add GOOD features** for more information\n",
    "   → These have <50% missing values\n",
    "   → Can improve model performance\n",
    "\n",
    "3️⃣ **Handle missing data:**\n",
    "   → For numeric: use median/mean imputation\n",
    "   → For categorical: use mode or create 'Unknown' category\n",
    "   → Or use algorithms that handle missing values (XGBoost, LightGBM)\n",
    "\n",
    "4️⃣ **Feature engineering:**\n",
    "   → Create date-based features (day, month, year, day of week)\n",
    "   → Combine categorical features\n",
    "   → Create interaction features\n",
    "\n",
    "5️⃣ **Encode categorical variables:**\n",
    "   → Label encoding for ordinal features\n",
    "   → One-hot encoding for nominal features\n",
    "   → Target encoding for high-cardinality features\n",
    "\n",
    "6️⃣ **Scale/normalize numeric features:**\n",
    "   → StandardScaler or MinMaxScaler\n",
    "   → Important for algorithms like SVM, Neural Networks\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"\\n🎉 Analysis complete! You now know:\")\n",
    "print(\"   ✅ Which dataset has the best features\")\n",
    "print(\"   ✅ Which datasets can be joined together\")\n",
    "print(\"   ✅ How to create a combined dataset\")\n",
    "print(\"   ✅ Which features to use for modeling\")\n",
    "print(\"\\n\" + \"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 💾 Section 11: Save the Combined Dataset\n",
    "\n",
    "Let's save our combined dataset for future use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the combined dataset\n",
    "output_file = 'combined_fontys_dataset.csv'\n",
    "combined_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"✅ Combined dataset saved to: {output_file}\")\n",
    "print(f\"   Size: {len(combined_df):,} rows × {len(combined_df.columns)} columns\")\n",
    "print(f\"   File size: {combined_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Also save the feature recommendations\n",
    "feature_scores_df.to_csv('feature_recommendations.csv', index=False)\n",
    "print(f\"\\n✅ Feature recommendations saved to: feature_recommendations.csv\")\n",
    "\n",
    "# Save recommended features list\n",
    "recommended_feature_names = recommended_features['Feature'].tolist()\n",
    "with open('recommended_features.txt', 'w') as f:\n",
    "    f.write(\"\\n\".join(recommended_feature_names))\n",
    "print(f\"✅ Recommended feature names saved to: recommended_features.txt\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"\\n🎉 ALL DONE! You're ready to start modeling!\")\n",
    "print(\"=\"*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
