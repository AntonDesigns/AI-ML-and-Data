{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Complete Feature Analysis - ALL Datasets Combined\n",
    "## Combining All 6 Datasets & Analyzing Which Features Are Best\n",
    "\n",
    "**Goal:** \n",
    "1. Combine ALL 6 datasets into ONE comprehensive dataset\n",
    "2. Get ALL features together\n",
    "3. Analyze which features have the best quality\n",
    "4. See which features work together (correlations)\n",
    "5. Identify the best features for modeling\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìÇ Section 1: Load ALL Datasets\n",
    "\n",
    "First, let's load all 6 datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Scanning for dataset files...\n",
      "\n",
      "================================================================================\n",
      "Found 0 dataset files:\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Loading all datasets...\n",
      "\n",
      "================================================================================\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Successfully loaded 0 datasets!\n",
      "\n",
      "‚ö†Ô∏è  WARNING: No datasets loaded! Please check:\n",
      "   1. Files are in the same directory as this notebook\n",
      "   2. Filenames contain the expected keywords\n",
      "   3. Files are valid Excel/CSV format\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Check multiple possible locations for the files\n",
    "possible_paths = [\n",
    "    '.',  # Current directory\n",
    "    '/mnt/user-data/uploads/',  # Uploads folder\n",
    "    '../',  # Parent directory\n",
    "    os.path.expanduser('~/Downloads/')  # User's downloads folder\n",
    "]\n",
    "\n",
    "print(\"üîç Scanning for dataset files...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "all_files = []\n",
    "data_path = None\n",
    "\n",
    "# Find which path has the files\n",
    "for path in possible_paths:\n",
    "    if os.path.exists(path):\n",
    "        files_found = glob.glob(os.path.join(path, '*.xlsx')) + \\\n",
    "                      glob.glob(os.path.join(path, '*.xls')) + \\\n",
    "                      glob.glob(os.path.join(path, '*.csv'))\n",
    "        if files_found:\n",
    "            all_files = files_found\n",
    "            data_path = path\n",
    "            print(f\"‚úÖ Found files in: {os.path.abspath(path)}\\n\")\n",
    "            break\n",
    "\n",
    "if not all_files:\n",
    "    print(\"‚ùå No files found! Please check file locations.\")\n",
    "else:\n",
    "    print(f\"Found {len(all_files)} files:\\n\")\n",
    "    for f in all_files:\n",
    "        print(f\"  üìÑ {os.path.basename(f)}\")\n",
    "\n",
    "# Define patterns to identify each dataset type\n",
    "dataset_patterns = {\n",
    "    'trip_flight': ['trip', 'flight'],\n",
    "    'deelnemer': ['deelnemer'],\n",
    "    'reizen_riss': ['reizen', 'riss'],\n",
    "    'overzicht_cax': ['overzicht', 'cax'],\n",
    "    'boekingen_total': ['boekingen', 'total'],\n",
    "    'grip_freeze': ['grip', 'freeze']\n",
    "}\n",
    "\n",
    "# Match files to dataset types\n",
    "files = {}\n",
    "for file in all_files:\n",
    "    filename = os.path.basename(file).lower()\n",
    "    \n",
    "    for dataset_name, patterns in dataset_patterns.items():\n",
    "        # Check if all patterns are in the filename\n",
    "        if all(pattern in filename for pattern in patterns):\n",
    "            files[dataset_name] = file\n",
    "            break\n",
    "\n",
    "datasets = {}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Loading all datasets...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, file in files.items():\n",
    "    try:\n",
    "        if file.endswith('.csv'):\n",
    "            # Try different encodings for CSV\n",
    "            try:\n",
    "                df = pd.read_csv(file, encoding='utf-8')\n",
    "            except:\n",
    "                try:\n",
    "                    df = pd.read_csv(file, encoding='latin-1')\n",
    "                except:\n",
    "                    try:\n",
    "                        df = pd.read_csv(file, encoding='utf-8', on_bad_lines='skip')\n",
    "                    except:\n",
    "                        df = pd.read_csv(file, encoding='ISO-8859-1')\n",
    "        else:\n",
    "            df = pd.read_excel(file)\n",
    "        \n",
    "        # Skip useless pivot tables\n",
    "        if len(df.columns) <= 2 and 'Unnamed' in ' '.join(df.columns):\n",
    "            print(f\"‚ö†Ô∏è  {name}: Skipped (pivot table, not useful)\")\n",
    "            continue\n",
    "            \n",
    "        datasets[name] = df\n",
    "        print(f\"‚úÖ {name}: {len(df):,} rows √ó {len(df.columns)} columns\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {name}: ERROR - {e}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n‚úÖ Successfully loaded {len(datasets)} datasets!\\n\")\n",
    "\n",
    "if len(datasets) == 0:\n",
    "    print(\"‚ö†Ô∏è  WARNING: No datasets loaded!\")\n",
    "    print(f\"\\nüìÇ Files found but not matched: {len(all_files) - len(files)}\")\n",
    "    if all_files and not files:\n",
    "        print(\"\\nüí° Try adjusting the pattern matching. Found these files:\")\n",
    "        for f in all_files:\n",
    "            print(f\"   - {os.path.basename(f)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîó Section 2: Identify Common Keys for Joining\n",
    "\n",
    "Let's find columns that can connect the datasets together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîç ANALYZING JOIN POSSIBILITIES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Show columns for each dataset\n",
    "for name, df in datasets.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Columns: {', '.join(df.columns[:15].tolist())}\")\n",
    "    if len(df.columns) > 15:\n",
    "        print(f\"           ... and {len(df.columns) - 15} more\")\n",
    "\n",
    "# Find common column names\n",
    "all_columns = {name: set(df.columns) for name, df in datasets.items()}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîó COMMON COLUMNS ACROSS DATASETS:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check for booking IDs or codes that might join datasets\n",
    "potential_keys = []\n",
    "\n",
    "# Look for common patterns in column names\n",
    "key_patterns = ['ID', 'id', 'code', 'Code', 'nummer', 'booking', 'Booking', 'reis', 'Reis']\n",
    "\n",
    "for ds1_name, cols1 in all_columns.items():\n",
    "    for ds2_name, cols2 in all_columns.items():\n",
    "        if ds1_name >= ds2_name:  # Skip duplicates\n",
    "            continue\n",
    "        \n",
    "        common = cols1 & cols2\n",
    "        if common:\n",
    "            print(f\"\\n{ds1_name} ‚Üî {ds2_name}:\")\n",
    "            print(f\"  Common: {', '.join(sorted(common))}\")\n",
    "            potential_keys.extend(list(common))\n",
    "\n",
    "potential_keys = list(set(potential_keys))\n",
    "print(f\"\\nüìå Potential join keys found: {', '.join(potential_keys) if potential_keys else 'None'}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Section 3: Create ONE Combined Dataset with ALL Features\n",
    "\n",
    "Let's merge all datasets to get ALL features in one place!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ CREATING COMBINED DATASET WITH ALL FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Strategy: Start with the largest dataset and try to merge others\n",
    "# If merging fails, we'll add datasets as separate rows (concatenate)\n",
    "\n",
    "# Find largest dataset\n",
    "largest_name = max(datasets.keys(), key=lambda k: len(datasets[k]))\n",
    "combined = datasets[largest_name].copy()\n",
    "\n",
    "print(f\"\\nüèÅ Starting with largest dataset: {largest_name}\")\n",
    "print(f\"   Initial: {len(combined):,} rows √ó {len(combined.columns)} columns\\n\")\n",
    "\n",
    "merged_datasets = [largest_name]\n",
    "failed_merges = []\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    if name == largest_name:\n",
    "        continue\n",
    "    \n",
    "    # Find best join key\n",
    "    common_cols = list(set(combined.columns) & set(df.columns))\n",
    "    \n",
    "    if not common_cols:\n",
    "        print(f\"‚ö†Ô∏è  {name}: No common columns, will add separately\")\n",
    "        failed_merges.append(name)\n",
    "        continue\n",
    "    \n",
    "    # Try each common column as join key\n",
    "    best_key = None\n",
    "    best_match_pct = 0\n",
    "    \n",
    "    for col in common_cols:\n",
    "        try:\n",
    "            # Calculate overlap\n",
    "            combined_values = set(combined[col].dropna().unique())\n",
    "            df_values = set(df[col].dropna().unique())\n",
    "            overlap = len(combined_values & df_values)\n",
    "            match_pct = (overlap / max(len(combined_values), len(df_values))) * 100\n",
    "            \n",
    "            if match_pct > best_match_pct:\n",
    "                best_match_pct = match_pct\n",
    "                best_key = col\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if best_key and best_match_pct > 10:  # At least 10% overlap\n",
    "        print(f\"‚úÖ Merging {name} on '{best_key}' ({best_match_pct:.1f}% match)\")\n",
    "        try:\n",
    "            before_cols = len(combined.columns)\n",
    "            combined = combined.merge(\n",
    "                df, \n",
    "                on=best_key, \n",
    "                how='outer',  # Keep all rows\n",
    "                suffixes=('', f'_{name}')\n",
    "            )\n",
    "            new_cols = len(combined.columns) - before_cols\n",
    "            print(f\"   ‚Üí Added {new_cols} new features\")\n",
    "            merged_datasets.append(name)\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Merge failed: {e}\")\n",
    "            failed_merges.append(name)\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  {name}: Poor match quality ({best_match_pct:.1f}%), will add separately\")\n",
    "        failed_merges.append(name)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"\\n‚úÖ Merged datasets: {', '.join(merged_datasets)}\")\n",
    "\n",
    "# For datasets that couldn't be merged, add them as separate rows\n",
    "if failed_merges:\n",
    "    print(f\"\\nüì¶ Adding remaining datasets as separate rows...\\n\")\n",
    "    \n",
    "    for name in failed_merges:\n",
    "        df = datasets[name]\n",
    "        # Align columns - add missing columns\n",
    "        for col in combined.columns:\n",
    "            if col not in df.columns:\n",
    "                df[col] = np.nan\n",
    "        for col in df.columns:\n",
    "            if col not in combined.columns:\n",
    "                combined[col] = np.nan\n",
    "        \n",
    "        # Concatenate\n",
    "        combined = pd.concat([combined, df[combined.columns]], ignore_index=True)\n",
    "        print(f\"‚úÖ Added {name}: +{len(df):,} rows\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nüéâ FINAL COMBINED DATASET:\")\n",
    "print(f\"   Total rows: {len(combined):,}\")\n",
    "print(f\"   Total features: {len(combined.columns)}\")\n",
    "print(f\"   Memory: {combined.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä Section 4: Analyze ALL Features Together\n",
    "\n",
    "Now let's analyze the quality of ALL features in our combined dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä ANALYZING ALL FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Comprehensive feature analysis\n",
    "feature_analysis = []\n",
    "\n",
    "for col in combined.columns:\n",
    "    # Basic metrics\n",
    "    total = len(combined)\n",
    "    missing = combined[col].isna().sum()\n",
    "    missing_pct = (missing / total) * 100\n",
    "    present = total - missing\n",
    "    present_pct = 100 - missing_pct\n",
    "    \n",
    "    # Unique values\n",
    "    unique = combined[col].nunique()\n",
    "    unique_pct = (unique / present) * 100 if present > 0 else 0\n",
    "    \n",
    "    # Data type\n",
    "    dtype = str(combined[col].dtype)\n",
    "    \n",
    "    # Determine feature type\n",
    "    if 'int' in dtype or 'float' in dtype:\n",
    "        feature_type = 'Numeric'\n",
    "    elif 'datetime' in dtype:\n",
    "        feature_type = 'DateTime'\n",
    "    else:\n",
    "        feature_type = 'Categorical'\n",
    "    \n",
    "    # Quality scoring\n",
    "    # 1. Completeness (70% weight) - lower missing is better\n",
    "    completeness_score = present_pct\n",
    "    \n",
    "    # 2. Usefulness (30% weight) - good variety, not too unique\n",
    "    if unique <= 1:\n",
    "        usefulness_score = 0  # Constant, useless\n",
    "    elif unique_pct > 95:\n",
    "        usefulness_score = 40  # Likely ID, less useful\n",
    "    elif 1 < unique_pct <= 50:\n",
    "        usefulness_score = 100  # Sweet spot!\n",
    "    else:\n",
    "        usefulness_score = 70\n",
    "    \n",
    "    quality_score = (completeness_score * 0.7) + (usefulness_score * 0.3)\n",
    "    \n",
    "    # Recommendation\n",
    "    if quality_score >= 80 and missing_pct < 30:\n",
    "        recommendation = \"‚úÖ EXCELLENT\"\n",
    "        color = 'green'\n",
    "    elif quality_score >= 60 and missing_pct < 50:\n",
    "        recommendation = \"üü¢ GOOD\"\n",
    "        color = 'lightgreen'\n",
    "    elif quality_score >= 40:\n",
    "        recommendation = \"üü° FAIR\"\n",
    "        color = 'yellow'\n",
    "    else:\n",
    "        recommendation = \"‚ùå POOR\"\n",
    "        color = 'red'\n",
    "    \n",
    "    feature_analysis.append({\n",
    "        'Feature': col,\n",
    "        'Type': feature_type,\n",
    "        'Data Type': dtype,\n",
    "        'Present': present,\n",
    "        'Missing': missing,\n",
    "        'Missing %': missing_pct,\n",
    "        'Unique': unique,\n",
    "        'Unique %': unique_pct,\n",
    "        'Quality Score': quality_score,\n",
    "        'Recommendation': recommendation,\n",
    "        'Color': color\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "features_df = pd.DataFrame(feature_analysis)\n",
    "features_df = features_df.sort_values('Quality Score', ascending=False)\n",
    "\n",
    "# Display formatted table\n",
    "display_df = features_df[['Feature', 'Type', 'Missing %', 'Unique', 'Quality Score', 'Recommendation']].copy()\n",
    "display_df['Missing %'] = display_df['Missing %'].apply(lambda x: f\"{x:.1f}%\")\n",
    "display_df['Quality Score'] = display_df['Quality Score'].apply(lambda x: f\"{x:.1f}\")\n",
    "\n",
    "print(\"\\nüèÜ ALL FEATURES RANKED BY QUALITY:\\n\")\n",
    "print(display_df.to_string(index=False))\n",
    "\n",
    "# Summary\n",
    "excellent = len(features_df[features_df['Recommendation'] == '‚úÖ EXCELLENT'])\n",
    "good = len(features_df[features_df['Recommendation'] == 'üü¢ GOOD'])\n",
    "fair = len(features_df[features_df['Recommendation'] == 'üü° FAIR'])\n",
    "poor = len(features_df[features_df['Recommendation'] == '‚ùå POOR'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nüìà FEATURE QUALITY SUMMARY:\")\n",
    "print(f\"   ‚úÖ EXCELLENT (>80 quality, <30% missing): {excellent} features\")\n",
    "print(f\"   üü¢ GOOD (>60 quality, <50% missing): {good} features\")\n",
    "print(f\"   üü° FAIR (>40 quality): {fair} features\")\n",
    "print(f\"   ‚ùå POOR (<40 quality): {poor} features\")\n",
    "print(f\"\\n   üí° RECOMMENDED FOR MODELING: {excellent + good} features\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä Section 5: Visualize Feature Quality\n",
    "\n",
    "Let's create visualizations to see feature quality distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Quality Score Distribution\n",
    "axes[0, 0].hist(features_df['Quality Score'], bins=20, color='#3498db', edgecolor='black')\n",
    "axes[0, 0].axvline(x=80, color='green', linestyle='--', linewidth=2, label='Excellent (80+)')\n",
    "axes[0, 0].axvline(x=60, color='orange', linestyle='--', linewidth=2, label='Good (60+)')\n",
    "axes[0, 0].axvline(x=40, color='red', linestyle='--', linewidth=2, label='Poor (<40)')\n",
    "axes[0, 0].set_xlabel('Quality Score', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Number of Features', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_title('üìä Feature Quality Score Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Recommendation Breakdown\n",
    "recommendation_counts = features_df['Recommendation'].value_counts()\n",
    "colors_pie = {'‚úÖ EXCELLENT': '#2ecc71', 'üü¢ GOOD': '#3498db', 'üü° FAIR': '#f39c12', '‚ùå POOR': '#e74c3c'}\n",
    "colors_list = [colors_pie.get(x, '#95a5a6') for x in recommendation_counts.index]\n",
    "axes[0, 1].pie(recommendation_counts.values, labels=recommendation_counts.index, \n",
    "               autopct='%1.1f%%', colors=colors_list, startangle=90)\n",
    "axes[0, 1].set_title('üéØ Feature Quality Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot 3: Missing Data by Feature Type\n",
    "type_missing = features_df.groupby('Type')['Missing %'].mean().sort_values(ascending=False)\n",
    "axes[1, 0].barh(type_missing.index, type_missing.values, color=['#e74c3c', '#3498db', '#f39c12'])\n",
    "axes[1, 0].set_xlabel('Average Missing %', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_title('üìâ Missing Data by Feature Type', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].grid(alpha=0.3, axis='x')\n",
    "\n",
    "# Plot 4: Top 15 Best Features\n",
    "top_features = features_df.head(15)\n",
    "colors_bar = [colors_pie.get(x, '#95a5a6') for x in top_features['Recommendation']]\n",
    "axes[1, 1].barh(range(len(top_features)), top_features['Quality Score'], \n",
    "                color=colors_bar, edgecolor='black')\n",
    "axes[1, 1].set_yticks(range(len(top_features)))\n",
    "axes[1, 1].set_yticklabels(top_features['Feature'], fontsize=9)\n",
    "axes[1, 1].set_xlabel('Quality Score', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_title('üèÜ Top 15 Best Features', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].invert_yaxis()\n",
    "axes[1, 1].grid(alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Visualizations created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîó Section 6: Analyze Which Features Work Together (Correlations)\n",
    "\n",
    "Let's see which features are related to each other!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîó ANALYZING FEATURE RELATIONSHIPS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get numeric features only\n",
    "numeric_features = features_df[features_df['Type'] == 'Numeric']['Feature'].tolist()\n",
    "numeric_data = combined[numeric_features]\n",
    "\n",
    "print(f\"\\nüìä Found {len(numeric_features)} numeric features\")\n",
    "\n",
    "if len(numeric_features) > 1:\n",
    "    # Calculate correlation matrix\n",
    "    correlation_matrix = numeric_data.corr()\n",
    "    \n",
    "    # Find strong correlations (>0.7 or <-0.7, excluding diagonal)\n",
    "    print(\"\\nüîç Strong Correlations (|correlation| > 0.7):\\n\")\n",
    "    \n",
    "    strong_correlations = []\n",
    "    for i in range(len(correlation_matrix)):\n",
    "        for j in range(i+1, len(correlation_matrix)):\n",
    "            corr_value = correlation_matrix.iloc[i, j]\n",
    "            if abs(corr_value) > 0.7 and not pd.isna(corr_value):\n",
    "                feature1 = correlation_matrix.index[i]\n",
    "                feature2 = correlation_matrix.columns[j]\n",
    "                strong_correlations.append({\n",
    "                    'Feature 1': feature1,\n",
    "                    'Feature 2': feature2,\n",
    "                    'Correlation': corr_value,\n",
    "                    'Strength': 'Very Strong' if abs(corr_value) > 0.9 else 'Strong'\n",
    "                })\n",
    "    \n",
    "    if strong_correlations:\n",
    "        corr_df = pd.DataFrame(strong_correlations)\n",
    "        corr_df = corr_df.sort_values('Correlation', key=abs, ascending=False)\n",
    "        corr_df['Correlation'] = corr_df['Correlation'].apply(lambda x: f\"{x:.3f}\")\n",
    "        print(corr_df.to_string(index=False))\n",
    "        \n",
    "        print(f\"\\nüí° Found {len(corr_df)} pairs of strongly correlated features!\")\n",
    "        print(\"   ‚Üí These features work together and provide similar information\")\n",
    "        print(\"   ‚Üí Consider keeping only one from each pair to avoid redundancy\")\n",
    "    else:\n",
    "        print(\"No strong correlations found (|r| > 0.7)\")\n",
    "        print(\"Most features are independent!\")\n",
    "    \n",
    "    # Create correlation heatmap for top features\n",
    "    if len(numeric_features) > 2:\n",
    "        print(\"\\nüìä Creating correlation heatmap...\")\n",
    "        \n",
    "        # Select top numeric features (by quality)\n",
    "        top_numeric = features_df[\n",
    "            (features_df['Type'] == 'Numeric') & \n",
    "            (features_df['Recommendation'].isin(['‚úÖ EXCELLENT', 'üü¢ GOOD']))\n",
    "        ]['Feature'].head(15).tolist()\n",
    "        \n",
    "        if len(top_numeric) > 1:\n",
    "            plt.figure(figsize=(12, 10))\n",
    "            \n",
    "            corr_matrix_top = combined[top_numeric].corr()\n",
    "            \n",
    "            # Create mask for upper triangle\n",
    "            mask = np.triu(np.ones_like(corr_matrix_top, dtype=bool))\n",
    "            \n",
    "            sns.heatmap(corr_matrix_top, mask=mask, annot=True, fmt='.2f', \n",
    "                       cmap='coolwarm', center=0, vmin=-1, vmax=1,\n",
    "                       square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "            \n",
    "            plt.title('üîó Feature Correlation Heatmap\\n(Top Quality Numeric Features)', \n",
    "                     fontsize=14, fontweight='bold', pad=20)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Not enough numeric features for correlation analysis\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Section 7: Final Feature Selection Recommendations\n",
    "\n",
    "Let's create the final list of recommended features for your model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üéØ FINAL FEATURE RECOMMENDATIONS FOR MODELING\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Get recommended features\n",
    "recommended = features_df[features_df['Recommendation'].isin(['‚úÖ EXCELLENT', 'üü¢ GOOD'])].copy()\n",
    "\n",
    "print(f\"\\n‚úÖ RECOMMENDED FEATURES ({len(recommended)} total):\\n\")\n",
    "\n",
    "# Show by category\n",
    "for feature_type in ['Numeric', 'Categorical', 'DateTime']:\n",
    "    type_features = recommended[recommended['Type'] == feature_type]\n",
    "    if len(type_features) > 0:\n",
    "        print(f\"\\n{feature_type} Features ({len(type_features)}):\")\n",
    "        print(\"-\" * 80)\n",
    "        for idx, row in type_features.iterrows():\n",
    "            print(f\"  {row['Recommendation']} {row['Feature']}\")\n",
    "            print(f\"       Quality: {row['Quality Score']:.1f}, Missing: {row['Missing %']:.1f}%, Unique: {row['Unique']}\")\n",
    "\n",
    "# Features to avoid\n",
    "avoid = features_df[features_df['Recommendation'] == '‚ùå POOR']\n",
    "print(f\"\\n\\n‚ùå FEATURES TO AVOID ({len(avoid)} total):\")\n",
    "print(\"-\" * 80)\n",
    "if len(avoid) > 0:\n",
    "    avoid_display = avoid[['Feature', 'Missing %', 'Quality Score']].head(10)\n",
    "    avoid_display['Missing %'] = avoid_display['Missing %'].apply(lambda x: f\"{x:.1f}%\")\n",
    "    avoid_display['Quality Score'] = avoid_display['Quality Score'].apply(lambda x: f\"{x:.1f}\")\n",
    "    print(avoid_display.to_string(index=False))\n",
    "    if len(avoid) > 10:\n",
    "        print(f\"\\n... and {len(avoid) - 10} more poor quality features\")\n",
    "    print(\"\\nüí° These features have too much missing data or are constant/IDs\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"\\nüìã SUMMARY:\")\n",
    "print(\"=\"*100)\n",
    "print(f\"‚úÖ Total features available: {len(features_df)}\")\n",
    "print(f\"‚úÖ Recommended for modeling: {len(recommended)}\")\n",
    "print(f\"‚ùå Not recommended: {len(avoid)}\")\n",
    "print(f\"\\nüìä Feature types in recommended set:\")\n",
    "print(f\"   - Numeric: {len(recommended[recommended['Type'] == 'Numeric'])}\")\n",
    "print(f\"   - Categorical: {len(recommended[recommended['Type'] == 'Categorical'])}\")\n",
    "print(f\"   - DateTime: {len(recommended[recommended['Type'] == 'DateTime'])}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"\\nüí° NEXT STEPS:\")\n",
    "print(\"=\"*100)\n",
    "print(\"\"\"\n",
    "1Ô∏è‚É£ **Use EXCELLENT features first** - These are your strongest predictors\n",
    "\n",
    "2Ô∏è‚É£ **Add GOOD features** - These will improve your model\n",
    "\n",
    "3Ô∏è‚É£ **Handle missing data:**\n",
    "   ‚Üí Numeric: Use median/mean imputation or algorithms that handle NaNs\n",
    "   ‚Üí Categorical: Use mode or create 'Unknown' category\n",
    "   ‚Üí Or try algorithms like XGBoost/LightGBM that handle missing data\n",
    "\n",
    "4Ô∏è‚É£ **Check correlations** - Remove redundant features that are highly correlated\n",
    "\n",
    "5Ô∏è‚É£ **Encode categorical features:**\n",
    "   ‚Üí One-hot encoding for few categories (<10)\n",
    "   ‚Üí Label encoding for ordinal data\n",
    "   ‚Üí Target encoding for many categories (>10)\n",
    "\n",
    "6Ô∏è‚É£ **Feature engineering from DateTime:**\n",
    "   ‚Üí Extract: year, month, day, day_of_week, quarter\n",
    "   ‚Üí Create: is_weekend, season, days_since_X\n",
    "\n",
    "7Ô∏è‚É£ **Scale numeric features** if needed (for algorithms like SVM, Neural Nets)\n",
    "\"\"\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üíæ Section 8: Save Results\n",
    "\n",
    "Let's save everything for future use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üíæ SAVING RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Save the combined dataset\n",
    "combined.to_csv('combined_all_datasets.csv', index=False)\n",
    "print(f\"\\n‚úÖ Combined dataset saved: 'combined_all_datasets.csv'\")\n",
    "print(f\"   Size: {len(combined):,} rows √ó {len(combined.columns)} columns\")\n",
    "\n",
    "# 2. Save feature analysis\n",
    "features_df.to_csv('feature_quality_analysis.csv', index=False)\n",
    "print(f\"\\n‚úÖ Feature analysis saved: 'feature_quality_analysis.csv'\")\n",
    "\n",
    "# 3. Save recommended features list\n",
    "recommended_list = recommended['Feature'].tolist()\n",
    "with open('recommended_features.txt', 'w') as f:\n",
    "    f.write('\\n'.join(recommended_list))\n",
    "print(f\"\\n‚úÖ Recommended features saved: 'recommended_features.txt'\")\n",
    "print(f\"   {len(recommended_list)} features\")\n",
    "\n",
    "# 4. Save features to avoid\n",
    "avoid_list = avoid['Feature'].tolist()\n",
    "with open('features_to_avoid.txt', 'w') as f:\n",
    "    f.write('\\n'.join(avoid_list))\n",
    "print(f\"\\n‚úÖ Features to avoid saved: 'features_to_avoid.txt'\")\n",
    "print(f\"   {len(avoid_list)} features\")\n",
    "\n",
    "# 5. Save correlation pairs if any\n",
    "if 'corr_df' in locals() and len(corr_df) > 0:\n",
    "    corr_df.to_csv('strong_correlations.csv', index=False)\n",
    "    print(f\"\\n‚úÖ Correlation analysis saved: 'strong_correlations.csv'\")\n",
    "    print(f\"   {len(corr_df)} correlation pairs\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nüéâ ALL DONE! You now have:\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "‚úÖ One combined dataset with ALL features from all 6 datasets\n",
    "‚úÖ Complete quality analysis of ALL features\n",
    "‚úÖ Clear recommendations on which features to use\n",
    "‚úÖ Analysis of which features work together (correlations)\n",
    "‚úÖ Everything saved for your model development!\n",
    "\"\"\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nüöÄ Ready to build your machine learning model!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
