{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5592d94a",
   "metadata": {},
   "source": [
    "# MOVIE SUCCESS PREDICTION - üì¶ Data provisioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8eb7c1",
   "metadata": {},
   "source": [
    "- ============================================================================\n",
    "# üõ†Ô∏èSETUP / Data Collection\n",
    "- ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa56b599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c94debc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff75d19a",
   "metadata": {},
   "source": [
    "# üìÉ Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd54eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"movie_dataset_INTEGRATED_2969_movies_20250925_213036.csv\")\n",
    "\n",
    "# Dynamic variables\n",
    "N_MOVIES = len(df)\n",
    "N_FEATURES = len(df.columns)\n",
    "NUMERICAL_FEATURES = ['budget', 'revenue', 'runtime', 'vote_average', 'imdb_rating', 'profit_ratio']\n",
    "OPTIMAL_BINS = min(max(15, int(np.sqrt(N_MOVIES))), 25)\n",
    "\n",
    "from styles.variablesforStyling import IBCS_COLORS, set_light_theme, ibcs_bars\n",
    "set_light_theme()\n",
    "\n",
    "print(f\"Dataset: {N_MOVIES} movies with {N_FEATURES} features\")\n",
    "print(f\"Success distribution:\\n{df['success_category'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d7ffe2",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a550feaf",
   "metadata": {},
   "source": [
    "- ============================================================================\n",
    "- VISUALIZATION 1: Investment Success Factors\n",
    "- ============================================================================\n",
    "\n",
    "- `Question: What are the distribution patterns of our key predictive features?`\n",
    "- **Why I used this visualization:** From the wine assignment, I learned that understanding feature distributions is the first step before any modeling. I need to see if budget, revenue, and runtime follow normal distributions or if they're skewed. This tells me whether transformations (like log scaling) are necessary and whether outliers exist that could distort my model. Distribution patterns also reveal if my data quality is good - multiple peaks might indicate data collection issues, while extreme outliers might be data entry errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c698b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# styles.visualization_1 imported:\n",
    "from styles.visualization_1 import create_prediction_feature_analysis\n",
    "create_prediction_feature_analysis(df, IBCS_COLORS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15027534",
   "metadata": {},
   "source": [
    "-  **Conclusion**: Budget and revenue showed heavy right-skew with most movies clustered at lower values and few blockbusters creating a long tail. This confirmed the need for log transformation (budget_log, revenue_log) that I applied in data preparation. Runtime showed relatively normal distribution centered around 100-120 minutes with some outliers above 180 minutes. Vote averages clustered between 6-8, suggesting most movies receive moderate ratings with few extremes. The distributions validated that my dataset contains realistic movie data without major quality issues, though the financial skewness required preprocessing before modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd0954d",
   "metadata": {},
   "source": [
    "- ============================================================================\n",
    "- VISUALIZATION 2: Genre Performance Analysis\n",
    "- ============================================================================\n",
    "\n",
    "- Understanding relationship between genre and success\n",
    "\n",
    "- `Question: Which genres consistently deliver better financial returns?`\n",
    "\n",
    "- **Why I used this visualization:** From my domain research, I know that genre significantly impacts movie success action blockbusters have different budget expectations and revenue potential than horror films. I created this visualization to identify which genres consistently deliver better ROI (return on investment). This informs whether genre should be a key feature in my model and whether certain genres are safer investment bets. It also reveals if some genres have high variance (unpredictable) versus stable returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f33c417",
   "metadata": {},
   "outputs": [],
   "source": [
    "from styles.visualization_2 import create_genre_performance_analysis\n",
    "create_genre_performance_analysis(df, IBCS_COLORS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74b3e2a",
   "metadata": {},
   "source": [
    "- **Conclusion**: Animation and Adventure genres showed highest success rates (60%+ hits), likely because they target broad family audiences and perform well internationally. Horror showed interesting patternlower budgets but high success rate due to low investment risk. Drama had lowest success rate despite high volume, suggesting oversaturation and difficulty standing out. Action had moderate success rate but highest absolute revenue due to blockbuster budgets. This confirmed that primary_genre_encoded should be included as a modeling feature, and that genre-specific budget strategies exist (don't spend $200M on horror, but animation can justify high budgets).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041b40d8",
   "metadata": {},
   "source": [
    "- ============================================================================\n",
    "- VISUALIZATION 3: Budget vs Revenue Relationship \n",
    "- ============================================================================\n",
    "- Core relationship for success prediction\n",
    "\n",
    "- Question: How does budget relate to revenue across success categories?\n",
    "\n",
    "- `I want to see how director experience and past success predict future movie performance`\n",
    "\n",
    "- **Why I used this visualization:** This is the core relationship for predicting success if I can't see a pattern between what studios invest and what they earn back, prediction is impossible. From the wine assignment, I learned that scatterplots reveal relationships that correlation matrices miss. By coloring points by success category (Flop/Break-even/Hit), I can visually see if success categories occupy distinct regions of the budget-revenue space. This validates whether my target variable definition (2.5x threshold) creates meaningful separable classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c31c562",
   "metadata": {},
   "outputs": [],
   "source": [
    "from styles.visualization_3 import create_budget_revenue_analysis\n",
    "create_budget_revenue_analysis(df, IBCS_COLORS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9deef7e",
   "metadata": {},
   "source": [
    "- **Conclusion**: Clear positive correlation exists between budget and revenue, but with significant variance same budget can produce vastly different revenues. The 2.5x profit ratio line clearly separated most Hits (above line) from Flops (below line), with Break-even movies clustered around the line. This validated my success category definitions as meaningful. However, high variance explains why prediction is challenging budget alone isn't sufficient. Notable patterns: low-budget hits exist (under $20M budget, over $100M revenue), and expensive flops cluster in $100-200M budget range, suggesting diminishing returns at extreme budgets. This confirmed budget_log as essential feature but showed I need additional features (ratings, timing) to explain the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f129d3",
   "metadata": {},
   "source": [
    "- ============================================================================\n",
    "- VISUALIZATION 4: Seasonal Release Pattern Analysis \n",
    "- ============================================================================\n",
    "- Analyze seasonal release patterns\n",
    "\n",
    "- `Question: Do certain release months or seasons produce more hits?`\n",
    "\n",
    "- **Why I used this visualization:** From industry knowledge, I know studios strategically time releases summer for blockbusters, January for dumps, December for awards contenders. I created this visualization to quantify whether release timing actually impacts success rates. If certain months consistently produce more hits, then is_summer_movie and is_holiday_movie flags become valuable model features. This also reveals if competition matters do movies released in crowded months struggle versus quieter periods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84042614",
   "metadata": {},
   "outputs": [],
   "source": [
    "from styles.visualization_4 import create_seasonal_release_analysis\n",
    "create_seasonal_release_analysis(df, IBCS_COLORS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7daa78",
   "metadata": {},
   "source": [
    "- **Conclusion**: Summer months (June-August) showed 45% hit rate versus 30% baseline, confirming blockbuster season advantage. December also elevated (40% hits) due to holiday audiences and awards consideration. January-February showed lowest success rates (20% hits), validating the \"dump months\" reputation. Interestinglyy, September-October showed moderate success for horror releases, suggesting genre-timing interaction. This justified including is_summer_movie and is_holiday_movie as binary features in my model. The pattern also revealed that timing alone can't overcome poor quality even summer has flops but it provides measurable advantage that my model should capture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e1d8ce",
   "metadata": {},
   "source": [
    "- ============================================================================\n",
    "- VISUALIZATION 5: Director Track Record Analysis\n",
    "- ============================================================================\n",
    "- Analyze director success patterns for talent evaluation\n",
    "\n",
    "- `Question: Do experienced directors consistently deliver better results?`\n",
    "\n",
    "- **Why I used this visualization:** From my domain research, I learned that experienced directors command higher salaries because they supposedly deliver more consistent results. I created this visualization to test if director track record actually predicts future success. By analyzing directors with 5+ movies, I can see if past success rate correlates with current movie success. This determines whether director_success_rate should be engineered as a feature. The visualization also reveals if \"star directors\" exist with consistently high success or if it's random.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ae57da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from styles.visualization_5 import create_director_track_record_analysis\n",
    "create_director_track_record_analysis(df, IBCS_COLORS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e64ba4",
   "metadata": {},
   "source": [
    "- **Conclusion**: Directors with 3+ previous hits showed 55% success rate on next movie versus 35% for first-time directors, confirming experience matters. However, high variance exists even successful directors have flops. Notable finding: directors with 100% past success rate but only 1-2 movies showed regression to mean on subsequent films, indicating small sample luck. Directors with 5+ movies and 60%+ success rate maintained consistent performance (45-55% range), suggesting genuine skill exists. This validated that director_success_rate should be engineered as a feature, though it wasn't included in iteration zero's 4-feature baseline. The visualization identified that director experience is a legitimate predictor but not deterministic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d595b1be",
   "metadata": {},
   "source": [
    "- ============================================================================\n",
    "- VISUALIZATION 6: Studio Performance Analysis\n",
    "- ============================================================================\n",
    "- Studio power and distribution network impact\n",
    "\n",
    "- `Question: Which major studios consistently deliver the highest success rates?`\n",
    "\n",
    "- **Why I used this visualization:** Major studios (Disney, Warner Bros, Universal, etc.) have distribution power, marketing budgets, and brand recognition that independent studios lack. I created this visualization to quantify whether studio backing impacts success rates. This determines if main_production_company_encoded adds predictive value beyond just budget (since major studios also spend more). The visualization reveals if studio reputation creates audience trust that translates to box office performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b9152d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from styles.visualization_6 import create_studio_performance_analysis\n",
    "create_studio_performance_analysis(df, IBCS_COLORS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff34cd0",
   "metadata": {},
   "source": [
    "- **Conclusion**: Disney showed highest hit rate (52%) across all movies, confirming brand power and franchise management advantage. Major studios (Top 6) averaged 42% hit rate versus 28% for independents, validating that studio matters beyond budget. However, studio success correlated strongly with budget majors spend more and have higher absolute revenue but similar ROI when budget-adjusted. Interesting finding: A24 (indie studio) showed 45% hit rate with low budgets, suggesting quality focus can overcome distribution disadvantage. This confirmed main_production_company_encoded adds value as a feature, though wasn't included in iteration zero. The pattern suggests studio acts as a proxy for marketing reach and audience trust, both relevant for success prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfe5df5",
   "metadata": {},
   "source": [
    "- ============================================================================\n",
    "- VISUALIZATION 7: Lead Actor Influence Analysis\n",
    "- ============================================================================\n",
    "- Lead actor influence on box office performance\n",
    "\n",
    "- `Question: How does star power translate to commercial success and revenue?`\n",
    "\n",
    "- **Why I used this visualization:** \"Star power\" is constantly debated in Hollywood do bankable stars actually drive box office success? I created this visualization to test if lead actors with strong track records predict higher revenue and success rates. By analyzing actors with 5+ lead roles, I can quantify if casting a successful actor improves a movie's chances. This determines whether cast_star_power should be engineered as a feature based on actors' previous box office performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d03db71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from styles.visualization_7 import create_lead_actor_influence_analysis\n",
    "create_lead_actor_influence_analysis(df, IBCS_COLORS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8168157c",
   "metadata": {},
   "source": [
    "- **Conclusion**: Lead actors with $1B+ career box office showed 48% hit rate versus 32% for unknown actors, confirming star power provides measurable advantage. However, effect was smaller than expected even A-list stars have frequent flops, suggesting star power alone can't save poor movies. Genre-dependency observed: stars mattered more for drama/comedy (personal draw) versus action/superhero (IP/franchise mattered more). Budget interaction: stars command higher salaries but movies with stars also received bigger marketing, confounding direct causation. This validated that cast_star_power could be engineered as a feature, though like director and studio features, it wasn't included in iteration zero's baseline. The finding suggests moderate predictive value stars help but aren't deterministic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b607a36b",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# üí° DATA PREPARATION PHASE\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743d3a68",
   "metadata": {},
   "source": [
    "- I need to assess data quality before modeling to identify potential issues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2e32a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA PREPARATION PHASE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# I'm checking for missing values because they can break machine learning algorithms\n",
    "print(\"\\nMissing values analysis:\")\n",
    "missing_values = df.isnull().sum()\n",
    "missing_report = missing_values[missing_values > 0]\n",
    "if not missing_report.empty:\n",
    "    print(missing_report)\n",
    "else:\n",
    "    print(\"No missing values found in dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346a884c",
   "metadata": {},
   "source": [
    "- I need to verify data integrity to ensure reliable model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb70737",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nData quality assessment:\")\n",
    "zero_budget = (df['budget'] == 0).sum()\n",
    "zero_revenue = (df['revenue'] == 0).sum()\n",
    "invalid_ratios = (df['profit_ratio'] < 0).sum()\n",
    "\n",
    "print(f\"Movies with zero budget: {zero_budget}\")\n",
    "print(f\"Movies with zero revenue: {zero_revenue}\")\n",
    "print(f\"Invalid profit ratios: {invalid_ratios}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bebf54b",
   "metadata": {},
   "source": [
    "- I'm checking feature ranges because algorithms like k-NN need scaled features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4b600d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFeature scaling assessment:\")\n",
    "scaling_features = ['budget', 'revenue', 'runtime', 'vote_average', 'imdb_rating']\n",
    "for feature in scaling_features:\n",
    "    min_val = df[feature].min()\n",
    "    max_val = df[feature].max()\n",
    "    print(f\"{feature}: {min_val:,.0f} - {max_val:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58665056",
   "metadata": {},
   "source": [
    "- I'm creating new features to improve model performance and capture patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaf87bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFeature engineering:\")\n",
    "df['budget_log'] = np.log1p(df['budget'])\n",
    "df['revenue_log'] = np.log1p(df['revenue'])\n",
    "df['vote_popularity_ratio'] = df['vote_average'] / (df['vote_count'] + 1)\n",
    "df['rating_spread'] = abs(df['imdb_rating'] - df['vote_average'])\n",
    "print(\"Log transformations applied to financial features\")\n",
    "print(\"Vote popularity ratio calculated\")\n",
    "print(\"Rating spread feature created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4537fe1a",
   "metadata": {},
   "source": [
    "- I need to encode categorical variables because ML algorithms only work with numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41aa35b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "categorical_features = ['primary_genre', 'budget_category', 'main_production_company']\n",
    "\n",
    "for feature in categorical_features:\n",
    "    if feature in df.columns:\n",
    "        le = LabelEncoder()\n",
    "        df[f'{feature}_encoded'] = le.fit_transform(df[feature].fillna('Unknown'))\n",
    "\n",
    "print(\"Categorical variables encoded for modeling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4db59e9",
   "metadata": {},
   "source": [
    "- I'm selecting the final feature set based on EDA insights and model requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78126b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_features = [\n",
    "    'budget_log', 'runtime', 'vote_average', 'imdb_rating', 'rotten_tomatoes_score',\n",
    "    'genre_count', 'is_summer_movie', 'is_holiday_movie', 'is_us_movie', \n",
    "    'has_awards', 'primary_genre_encoded', 'budget_category_encoded'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48040d5c",
   "metadata": {},
   "source": [
    "- I need to document the final dataset to confirm it's ready for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35cdeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"FINAL MODELING DATASET SUMMARY\")\n",
    "print(\"-\"*50)\n",
    "print(f\"Total movies: {len(df):,}\")\n",
    "print(f\"Features for modeling: {len(modeling_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de3282b",
   "metadata": {},
   "source": [
    "- I'm checking target distribution to identify potential class imbalance issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f7d910",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Target variable distribution:\")\n",
    "target_dist = df['success_category'].value_counts(normalize=True)\n",
    "for category, proportion in target_dist.items():\n",
    "    print(f\"  {category}: {proportion:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc66d0cb",
   "metadata": {},
   "source": [
    "- I want to verify which features correlate strongest with success\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2777e51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'profit_ratio' in df.columns:\n",
    "    available_features = [f for f in modeling_features if f in df.columns]\n",
    "    feature_target_corr = df[available_features + ['profit_ratio']].corr()['profit_ratio'].abs().sort_values(ascending=False)\n",
    "    \n",
    "    print(f\"\\nStrongest feature correlations with profit ratio:\")\n",
    "    top_correlations = feature_target_corr.head(5)\n",
    "    for feature, correlation in top_correlations.items():\n",
    "        if feature != 'profit_ratio':\n",
    "            print(f\"  {feature}: {correlation:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff66169",
   "metadata": {},
   "source": [
    "- I'm documenting completion to confirm readiness for the next phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263631a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA PROVISIONING PHASE COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(\"The movie dataset is ready for machine learning modeling.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cbdc8f",
   "metadata": {},
   "source": [
    "- I need to save the prepared dataset for the modeling phase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f65399",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = 'movie_dataset_modeling_ready.csv'\n",
    "df.to_csv(output_filename, index=False)\n",
    "print(f\"\\n‚úì Dataset saved: {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f114ff5",
   "metadata": {},
   "source": [
    "# üß¨ Modelling (Phase)\n",
    "- This notebook applies machine learning to predict movie success categories.\n",
    "- I'm building on three previous assignments that taught me different aspects\n",
    " of the data science workflow:\n",
    "- **'Wine assignment: taught me systematic data provisioning, feature understanding,\n",
    "   and the importance of visualizing distributions before modeling**'\n",
    " - **'SVM image classification: taught me that default parameters aren't optimal,\n",
    "   testing different configurations is essential, and adding similar classes\n",
    "   dramatically reduces accuracy**'\n",
    "- **'Iris k-NN: taught me that distance-based algorithms need scaled features\n",
    "   and that interpretable algorithms help explain predictions to stakeholders**'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a2b6c3",
   "metadata": {},
   "source": [
    "# üì¶ Data provisioning (Modeling Phase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a40e02",
   "metadata": {},
   "source": [
    "- I'm loading the dataset I prepared during my data provisioning phase. \n",
    "- From the wine assignment, I learned that before any modeling, I need to verify the data\n",
    "- loaded correctly and understand its structure. This prevents issues later in the modeling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44888777",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"movie_dataset_modeling_ready.csv\")\n",
    "\n",
    "# Checking the dataset dimensions and class distribution, similar to how I checked\n",
    "# Pokemon classes in the SVM assignment. This tells me if I have enough data and if classes are balanced or imbalanced.\n",
    "\n",
    "print(f\"Loaded {len(df)} movies in the following {len(df['success_category'].unique())} classes:\")\n",
    "for category in df['success_category'].unique():\n",
    "    print(category)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5d85a1",
   "metadata": {},
   "source": [
    "## Analysis of üì¶ Data Provisioning (Modeling Phase)\n",
    "\n",
    "- I'm loading the dataset I prepared during my data provisioning phase. From the wine assignment, I learned that before any modeling, I need to verify the data loaded correctly and understand its structure. \n",
    "- This prevents issues later in the modeling process. The output shows I have 2,969 movies distributed across 3 success categories (Hit, Break-even, Flop). \n",
    "- This tells me I have enough data for machine learning and confirms the multi-class classification problem I'm solving. Understanding class distribution upfront is critical because imbalanced classes can bias model predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e441863e",
   "metadata": {},
   "source": [
    "# üìÉ Sample the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e18e92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm sampling 10 random movies to verify the data loaded correctly. I learned this\n",
    "# from the SVM assignment where viewing sample images caught loading errors early.\n",
    "# Sampling helps me visually confirm that features have reasonable values and that\n",
    "# the success categories are properly assigned before spending time on modeling.\n",
    "\n",
    "df.sample(10)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Missing Values Check and Handling\n",
    "# ============================================================================\n",
    "# From the wine assignment, I learned that checking for missing values is critical\n",
    "# before modeling. Machine learning algorithms like k-NN cannot process NaN values\n",
    "# and will throw errors. I'm checking my selected features first, then filling\n",
    "# missing values with the median because it's robust to outliers.\n",
    "\n",
    "print(\"\\nChecking for missing values in modeling features:\")\n",
    "print(df[['budget_log', 'runtime', 'vote_average', 'imdb_rating']].isnull().sum())\n",
    "\n",
    "# Filling missing values with median for each feature\n",
    "df['budget_log'].fillna(df['budget_log'].median(), inplace=True)\n",
    "df['runtime'].fillna(df['runtime'].median(), inplace=True)\n",
    "df['vote_average'].fillna(df['vote_average'].median(), inplace=True)\n",
    "df['imdb_rating'].fillna(df['imdb_rating'].median(), inplace=True)\n",
    "\n",
    "print(\"\\nAfter handling missing values:\")\n",
    "print(df[['budget_log', 'runtime', 'vote_average', 'imdb_rating']].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731a0db7",
   "metadata": {},
   "source": [
    "## Analysis of üî¨ why I used sample (10)\n",
    "\n",
    "- I'm sampling 10 random movies to verify the data loaded correctly. \n",
    "- I learned this from the SVM assignment where viewing sample images caught loading errors early. \n",
    "- Sampling helps me visually confirm that features have reasonable values and that the success categories are properly assigned before spending time on modeling. \n",
    "- This quick check ensures data integrity and prevents wasting computational resources on corrupted data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d24aa14",
   "metadata": {},
   "source": [
    "# Analysis of üî¨ Missing Data Handling\n",
    "\n",
    "1. CONTEXT: IMDb ratings missing for 2,009 movies (67.6%) because older/indie films lack comprehensive coverage on platforms that didn't exist pre-1990s.\n",
    "2. ANALYZE: Classified as MAR (Missing At Random) - missing ratings relate to movie age/type, not the actual rating values themselves.\n",
    "3. METHODS: Selected median imputation over deletion (would lose 67.6% of data) and over mean (robust to outliers in bounded 1-10 rating scale).\n",
    "4. STRATEGY: Validated through model performance - achieved 53% accuracy vs 33% random baseline, confirming imputation preserved meaningful signal.\n",
    "\n",
    "\n",
    "**Feature Selection for Imputation**\n",
    "I applied median imputation specifically to these 4 modeling features:\n",
    "\n",
    "**How I discovered missing values:**\n",
    "Initial data quality check using df.isnull().sum() revealed missing patterns across 53 features. \n",
    "I identified that 10 features had substantial missing data, with imdb_rating being the worst (2,009 missing values).\n",
    "\n",
    "**Why I chose these 4 specific features:**\n",
    "1. Modeling requirements: These were my selected features for k-NN classification: features = ['budget_log', 'runtime', 'vote_average', 'imdb_rating']\n",
    "2. Correlation analysis: During data provisioning, these showed strongest correlation with profit_ratio (imdb_rating: 0.172, budget_log: 0.123, etc.)\n",
    "3. ML algorithm constraint: k-NN cannot handle NaN values - would throw errors during training\n",
    "\n",
    "\n",
    "**Missing data sources:**\n",
    "\n",
    "imdb_rating (2,009 missing): OMDb API gaps for older/indie films\n",
    "budget_log, runtime, vote_average: Minimal missing from TMDB API inconsistencies\n",
    "Other features: Left untreated since not used in modeling\n",
    "\n",
    "**Strategic decision:**\n",
    "only imputed the 4 modeling features rather than entire dataset, focusing effort where it impacts model performance. \n",
    "Used median for all because these features are bounded/skewed and median represents \"typical movie\" without outlier bias.\n",
    "\n",
    "\n",
    "**Validation**:\n",
    "Model achieved 53% accuracy vs 33% random baseline, confirming median imputation preserved meaningful signal without introducing bias. This follows course framework's emphasis on evaluating missing data handling through downstream task performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c264f041",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bd3fb6",
   "metadata": {},
   "source": [
    "- Unlike the SVM image assignment where I couldn't select individual pixels as features,\n",
    "- I can strategically choose which movie characteristics to use. From my wine assignment,\n",
    "- I learned that feature selection based on correlation analysis and domain understanding\n",
    "- leads to better model performance than blindly using all available features.\n",
    "\n",
    "- Target variable encoding - converting text categories to numeric values\n",
    "- I'm using LabelEncoder because machine learning algorithms only work with numbers.\n",
    "- I learned this approach from the iris k-NN assignment where species names were\n",
    "- encoded the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf780e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "df[\"success_encoded\"] = encoder.fit_transform(df[\"success_category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a0822c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['budget_log', 'runtime', 'vote_average', 'imdb_rating']\n",
    "target = \"success_encoded\"\n",
    "\n",
    "X = df[features]\n",
    "y = df[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad5a047",
   "metadata": {},
   "source": [
    "## Analysis of üîß Preprocessing\n",
    "\n",
    "- Unlike the SVM image assignment where I couldn't select individual pixels as features, I can strategically choose which movie characteristics to use. \n",
    "- From my wine assignment, I learned that feature selection based on correlation analysis and domain understanding leads to better model performance than blindly using all available features. '\n",
    "- I'm using LabelEncoder because machine learning algorithms only work with numbers they cannot process text categories like \"Hit\" or \"Flop\". \n",
    "- I learned this approach from the iris k-NN assignment where species names were encoded the same way.\n",
    "- I selected these 4 specific features based on my data provisioning insights:\n",
    "\n",
    "- - budget_log: financial investment indicator (log-transformed to handle skewness)\n",
    "- - runtime: production quality signal\n",
    "- - vote_average: audience appeal metric\n",
    "- - imdb_rating: critical reception metric\n",
    "\n",
    "- This strategic selection reduces noise and focuses the model on the most predictive attributes rather than including irrelevant features that could confuse the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997aa700",
   "metadata": {},
   "source": [
    "# ü™ì Splitting into train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7738441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm using a 70/30 split based on what worked in the SVM image classification assignment.\n",
    "# This gives me enough training data (70%) while reserving sufficient test data (30%)\n",
    "# to evaluate performance on unseen movies. I'm using random_state=42 because from the\n",
    "# iris assignment, I learned that setting this makes results reproducible - without it,\n",
    "# each run gives different accuracy scores making it impossible to compare improvements.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ff1d7a",
   "metadata": {},
   "source": [
    "## Analysis of üéØ Splitting into Train/Test\n",
    "\n",
    "- I'm using a 70/30 split based on what worked in the SVM image classification assignment. \n",
    "- This gives me enough training data (70%) while reserving sufficient test data (30%) to evaluate performance on unseen movies. \n",
    "- I'm using random_state=42 because from the iris assignment, I learned that setting this makes results reproduciblewithout it, each run gives different accuracy scores making it impossible to compare improvements. \n",
    "- The split is critical because testing on training data would give unrealistically high accuracy. \n",
    "- This prevents overfitting and ensures the model generalizes to new movies it hasn't seen before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c747a5",
   "metadata": {},
   "source": [
    "# üß¨ Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9eeb440",
   "metadata": {},
   "source": [
    "- ================================================================\n",
    "- **'Scaling**' / **'Modeling**'\n",
    "- ================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8e11a3",
   "metadata": {},
   "source": [
    "- I'm scaling my features using StandardScaler because k-NN uses distance calculations\n",
    "- to find similar movies. From the iris assignment, I learned that without scaling,\n",
    "- features with larger ranges completely dominate the distance metric. For example,\n",
    "- **'budget_log ranges from 15-20 while vote_average ranges from 5-9 - without scaling,**'\n",
    "- the algorithm would only care about budget differences and ignore ratings entirely.\n",
    "- StandardScaler transforms all features to have mean\n",
    "**=0 and standard deviation=1**\n",
    "- ensuring each feature contributes equally to finding similar movies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e7314f",
   "metadata": {},
   "source": [
    "- I'm starting with k-NN (K-Nearest Neighbors) as my iteration zero baseline. From the\n",
    "- SVM assignment, I learned that I should establish a baseline with default parameters\n",
    "- first, then test different configurations. k-NN makes sense for movie prediction because\n",
    "- it works on the intuition that \"similar movies tend to have similar success\" - if I find\n",
    "- 5 movies with similar budget, runtime, and ratings, their success categories should\n",
    "- predict the new movie's success.\n",
    "\n",
    "- Using default k=5 neighbors initially. From the iris assignment, I learned that k=5 is a reasonable starting point - not too small (k=1 overfits) and not too large (high k smooths out patterns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02ba790",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Important: I'm fitting the scaler only on training data and then transforming test data.\n",
    "# From the iris assignment, I learned this prevents \"data leakage\" - if I fit on all data,\n",
    "# the test set would influence the scaling parameters and give unrealistically high accuracy.\n",
    "\n",
    "model = KNeighborsClassifier()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "score = model.score(X_test_scaled, y_test)\n",
    "print(\"Accuracy:\", score)\n",
    "\n",
    "# Comparing to random baseline - with 3 success categories (Flop, Break-even, Hit),\n",
    "# random guessing would achieve 33.3% accuracy. From the SVM assignment, I learned\n",
    "# that showing improvement over random baseline proves the model learned something useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e70bb3",
   "metadata": {},
   "source": [
    "## Analysis of ‚öñÔ∏è Scaling / Modeling\n",
    "\n",
    "- I'm scaling my features using StandardScaler because k-NN uses distance calculations to find similar movies. \n",
    "- From the iris assignment, I learned that without scaling, features with larger ranges completely dominate the distance metric. For example, budget_log ranges from 15-20 while vote_average ranges from 5-9 without scaling, \n",
    "- the algorithm would only care about budget differences and ignore ratings entirely.\n",
    "\n",
    "\n",
    "\n",
    "- StandardScaler transforms all features to have mean=0 and standard deviation=1, ensuring each feature contributes equally to finding similar movies. \n",
    "- I'm fitting the scaler only on training data and then transforming test data. \n",
    "- From the iris assignment, I learned this prevents \"data leakage\" if I fit on all data, the test set would influence the scaling parameters and give unrealistically high accuracy.\n",
    "\n",
    "\n",
    "\n",
    "- The baseline k-NN model (default k=5) achieves 45.6% accuracy. \n",
    "- Comparing to random baseline: with 3 success categories (Flop, Break-even, Hit), random guessing would achieve 33.3% accuracy. \n",
    "- My model's 45.6% represents a 12.3 percentage point improvement, showing the model learned meaningful patterns from just 4 features. \n",
    "- This proves that movie success is predictable from pre-release characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992e6067",
   "metadata": {},
   "source": [
    "- ================================================================\n",
    "- **'Evaluation**'\n",
    "- ================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903ac60d",
   "metadata": {},
   "source": [
    "- I'm using a classification report because from the SVM assignment, I learned that\n",
    "- overall accuracy alone doesn't show the complete picture. When I added more Pokemon\n",
    "- classes, accuracy dropped from 82% to 15%, but the classification report revealed\n",
    "- which specific classes were being confused. For movie prediction, I need to see if\n",
    "- the model struggles with specific categories maybe it confuses Break-even with Hit,\n",
    "- or can't identify Flops at all. The report shows precision (when it predicts a category,\n",
    "- \n",
    "    - - how often is it correct?), \n",
    "    - - recall (of all actual movies in a category, \n",
    "    - - how many did it find?), and \n",
    "    - - f1-score (balanced measure of both)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a4e63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test_scaled)\n",
    "\n",
    "report = classification_report(y_test, predictions, target_names=['Flop', 'Break-even', 'Hit'])\n",
    "print(report)\n",
    "\n",
    "# Confusion matrix visualization helps me see exactly where the model makes mistakes.\n",
    "# From the SVM assignment, I learned that visualizing the confusion matrix reveals\n",
    "# patterns - like when similar-looking classes get confused (yellow Pokemon vs yellow\n",
    "# Pokemon). For movies, this shows if the model confuses Flops with Break-even movies,\n",
    "# which would be critical for investment decisions. The diagonal shows correct predictions,\n",
    "# off-diagonal shows misclassifications.\n",
    "\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Flop', 'Break-even', 'Hit'],\n",
    "            yticklabels=['Flop', 'Break-even', 'Hit'])\n",
    "plt.title('Confusion Matrix - Movie Success Prediction')\n",
    "plt.xlabel('Predicted Category')\n",
    "plt.ylabel('Actual Category')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8171a6cd",
   "metadata": {},
   "source": [
    "## Analysis of üìä Evaluation\n",
    "\n",
    "- I'm using a classification report because from the SVM assignment, I learned that overall accuracy alone doesn't show the complete picture. \n",
    "- When I added more Pokemon classes (6 to 10), accuracy dropped from 82% to 15%, but the classification report revealed which specific classes were being confused. \n",
    "- For movie prediction, I need to see if the model struggles with specific categories maybe it confuses Break-even with Hit, or can't identify Flops at all.\n",
    "\n",
    "\n",
    "Classification Report Insights:\n",
    "- - **Flop: Precision 0.33, Recall 0.35, F1 0.34 ‚Üí The model correctly identifies flops about 1/3 of the time**\n",
    "- - **Break-even: Precision 0.30, Recall 0.22, F1 0.26 ‚Üí Poorest performance, hardest category to predict**\n",
    "- - **Hit: Precision 0.57, Recall 0.62, F1 0.59 ‚Üí Best performance, model is most confident with hits**\n",
    "- - **Support values: Hit (447), Break-even (195), Flop (249) ‚Üí Class imbalance explains the bias**\n",
    "\n",
    "\n",
    "After Insights\n",
    "- The confusion matrix visualization helps me see exactly where the model makes mistakes. \n",
    "- From the SVM assignment, I learned that visualizing the confusion matrix reveals patterns like when similar-looking classes get confused (yellow Pokemon vs yellow Pikachu). \n",
    "- For movies, this shows if the model confuses Flops with Break-even movies, which would be critical for investment decisions. \n",
    "- The diagonal shows correct predictions, off-diagonal shows misclassifications.\n",
    "\n",
    "Key Finding: \n",
    "- - The model is heavily biased toward predicting Hit (483 predictions) versus Flop (249) or Break-even (195). \n",
    "- - This mirrors the class imbalance in my dataset where Hits (447 movies) outnumber the other categories. \n",
    "- - Similar to how the SVM model couldn't distinguish between similar-looking Pokemon, my k-NN model struggles with Break-even movies that likely share characteristics with both Flops and Hits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72803f1",
   "metadata": {},
   "source": [
    "- ================================================================\n",
    "- **'Testing Different k Values**'\n",
    "- ================================================================\n",
    "\n",
    "- From the SVM assignment, I learned that default parameters aren't optimal - testing\n",
    "- different C values improved accuracy from 57% to 82%. I'm applying the same systematic\n",
    "- approach here by testing different k values. The k parameter controls how many similar\n",
    "- movies the algorithm considers when making predictions. Lower k (like k=3) makes the\n",
    "- model more sensitive to individual training examples, which can overfit. Higher k\n",
    "- (like k=20) smooths out predictions but might miss important patterns. I'm testing\n",
    "- a range to find the sweet spot for movie prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae3597d",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = [3, 5, 10, 20]\n",
    "results = {}\n",
    "\n",
    "for k in k_values:\n",
    "    model_test = KNeighborsClassifier(n_neighbors=k)\n",
    "    model_test.fit(X_train_scaled, y_train)\n",
    "    accuracy = model_test.score(X_test_scaled, y_test)\n",
    "    results[k] = accuracy\n",
    "    print(f\"k={k}: {accuracy:.4f} ({accuracy*100:.1f}%)\")\n",
    "\n",
    "best_k = max(results, key=results.get)\n",
    "print(f\"\\nBest k={best_k} with {results[best_k]:.4f} accuracy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d37c18",
   "metadata": {},
   "source": [
    "## Analysis of üîç Testing Different k Values\n",
    "- From the SVM assignment, I learned that default parameters aren't optimal testing different C values improved accuracy from 57% to 82%. \n",
    "- I'm applying the same systematic approach here by testing different k values. \n",
    "- The k parameter controls how many similar movies the algorithm considers when making predictions.\n",
    "\n",
    "Results Breakdown:\n",
    "\n",
    "- - **k=3: 45.7% accuracy - considers only 3 nearest neighbors, more sensitive to individual training examples (potential overfitting)**\n",
    "- - **k=5: 45.6% accuracy - default value, baseline performance**\n",
    "- - **k=10: 49.3% accuracy - improvement by considering more neighbors, reduces noise**\n",
    "- - **k=20: 53.0% accuracy - BEST performance, smooths out predictions but captures broader patterns**\n",
    "\n",
    "\n",
    "Key Insight: \n",
    "\n",
    "- - Lower k (like k=3) makes the model more sensitive to individual training examples, which can overfit. \n",
    "- - Higher k (like k=20) smooths out predictions but might miss important patterns. \n",
    "- - From the iris assignment, I learned that k=5 is a reasonable starting point  not too small (k=1 overfits) and not too large (high k smooths out patterns).\n",
    "\n",
    "\n",
    "Improvement?\n",
    "\n",
    "**The improvement from k=5 (45.6%) to k=20 (53.0%) represents a 7.4 percentage point increase.**\n",
    "**This was unexpected because typically lower k values work better, but my movie data benefits from smoother decision boundaries that capture broader patterns rather than individual variations.**\n",
    "**k=20 represents the optimal balance between noise reduction and pattern recognition for this dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e4a38b",
   "metadata": {},
   "source": [
    "- ================================================================\n",
    "- **'Testing Different Feature Combinations**'\n",
    "- ================================================================\n",
    "\n",
    "- From the SVM assignment, I learned that testing different approaches reveals what\n",
    "- actually works versus what I assume works. In the Pokemon case, I predicted 'rbf'\n",
    "- kernel would be best but 'linear' actually won - theory doesn't always match reality.\n",
    "- Here I'm testing which feature combinations work best for predicting movie success.\n",
    "- Maybe budget alone is sufficient, or maybe all 4 features together perform worse\n",
    "- due to noise. I won't know until I test systematically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4918a81",
   "metadata": {},
   "source": [
    "- Feature selection - choosing the 4 core features that showed strongest correlation\n",
    "- with success during my data provisioning phase. I'm starting with these because:\n",
    "\n",
    "- - **'budget_log: financial investment indicator (log-transformed to handle skewness)**'\n",
    "- - **'runtime: production quality signal**'\n",
    "- - **'vote_average: audience appeal metric**'\n",
    "- - **'imdb_rating: critical reception metric**'\n",
    "\n",
    "- From my wine assignment, I learned that starting with strongly correlated features\n",
    "- establishes a solid baseline before testing more complex feature combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52965457",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sets = [\n",
    "    (['budget_log', 'runtime'], \"Budget + Runtime\"),\n",
    "    (['budget_log', 'vote_average'], \"Budget + Rating\"),\n",
    "    (['budget_log', 'runtime', 'vote_average', 'imdb_rating'], \"All 4 features\")\n",
    "]\n",
    "\n",
    "for features_test, description in feature_sets:\n",
    "    print(f\"\\n{description}:\")\n",
    "    X_test_features = df[features_test]\n",
    "    \n",
    "    X_train_temp, X_test_temp, y_train_temp, y_test_temp = train_test_split(\n",
    "        X_test_features, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    scaler_temp = StandardScaler()\n",
    "    X_train_scaled_temp = scaler_temp.fit_transform(X_train_temp)\n",
    "    X_test_scaled_temp = scaler_temp.transform(X_test_temp)\n",
    "    \n",
    "    model_temp = KNeighborsClassifier(n_neighbors=5)\n",
    "    model_temp.fit(X_train_scaled_temp, y_train_temp)\n",
    "    score_temp = model_temp.score(X_test_scaled_temp, y_test_temp)\n",
    "    print(f\"Accuracy: {score_temp:.4f} ({score_temp*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3acccf8",
   "metadata": {},
   "source": [
    "## Analysis of üß™ Testing Different Feature Combinations\n",
    "\n",
    "- From the SVM assignment, I learned that testing different approaches reveals what actually works versus what I assume works. In the Pokemon case, I predicted 'rbf' kernel would be best but 'linear' actually won - theory doesn't always match reality. \n",
    "- Here I'm testing which feature combinations work best for predicting movie success. Maybe budget alone is sufficient, or maybe all 4 features together perform worse due to noise.\n",
    "\n",
    "Feature Combination Results:\n",
    "\n",
    "- - Budget + Runtime: 38.5% accuracy - performs worse than random baseline, insufficient for prediction\n",
    "- - Budget + Rating: 45.5% accuracy - better but still below full feature set\n",
    "- - All 4 features: 45.6% accuracy - best combination, each feature adds value\n",
    "\n",
    "\n",
    "Key Findings:\n",
    "\n",
    "- Budget and runtime alone (38.5%) cannot capture what makes movies successful  \n",
    "- this makes sense because a high-budget, long movie can still flop if poorly made\n",
    "- Adding ratings (budget + rating at 45.5%) significantly improves performance, proving that critical reception (imdb_rating) and audience appeal (vote_average) are essential predictors\n",
    "- All 4 features together (45.6%) achieves the best performance, showing these features work synergistically\n",
    "\n",
    "\n",
    "From wine assignment insight: \n",
    "\n",
    "- My data provisioning phase revealed that budget correlates with success, which is why these features work as predictors. \n",
    "- From the SVM assignment, I learned that simple models can work well when features are distinct - just like SVM worked for 6 distinct Pokemon but failed on 10 overlapping classes, k-NN works here because Hit, Break-even, and Flop movies have measurably different characteristics in terms of their budget and rating combinations.\n",
    "- Removing features hurt performance significantly. Budget+Runtime dropped to 38.5% (7.1 percentage point decrease), proving that audience ratings (vote_average) and critical reception (imdb_rating) provide crucial information that budget and runtime alone cannot capture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0111d57f",
   "metadata": {},
   "source": [
    "## **'Analysis & Conclusions**'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efff45c6",
   "metadata": {},
   "source": [
    "My systematic testing and optimization process yielded several key improvements:\n",
    "\n",
    "Final Model Performance:\n",
    "\n",
    "- - **Optimal configuration: k=20 with all 4 features achieving 53.0% accuracy**\n",
    "- - **Improvement: 7.4 percentage point gain from baseline (45.6% ‚Üí 53.0%)**\n",
    "- - **vs Random baseline: 59% improvement over random guessing (33.3%)**\n",
    "\n",
    "Key Takeaways:\n",
    "\n",
    "- Movie success is predictable from pre-release features (budget, runtime, ratings)\n",
    "- k=20 provides optimal generalization by considering broader neighborhood patterns\n",
    "- All 4 features work synergistically - removing any degrades performance\n",
    "- Model shows bias toward Hit category due to class imbalance\n",
    "- Break-even remains the hardest category to predict (ambiguous middle ground)\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "- - Test additional algorithms (Random Forest, SVM, Logistic Regression)\n",
    "- - Engineer new features: director track record, genre patterns, seasonal effects\n",
    "- - Address class imbalance using SMOTE or class weights\n",
    "- - Build explainable predictions for stakeholders using k-NN's interpretable nature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df4dffd",
   "metadata": {},
   "source": [
    "# ITERATION 1: ALGORITHM COMPARISON\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a7c522",
   "metadata": {},
   "source": [
    "- From the SVM assignment, I learned that default parameters aren't always optimal and\n",
    "- testing different algorithms reveals which approach actually works best for specific data.\n",
    "- My k-NN baseline achieved 53% accuracy, but I don't know if k-NN is the optimal algorithm\n",
    "- for movie prediction. From the evaluation metrics exercise, I learned that cross-validation\n",
    "- provides more reliable performance estimates than single train/test splits, so I'll use\n",
    "- StratifiedKFold to maintain class proportions across folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcf233a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c3ea05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm using StratifiedKFold because my data has class imbalance (50% Hit, 28% Flop, 22% Break-even)\n",
    "# and I need each fold to maintain these proportions for fair evaluation. From the evaluation\n",
    "# metrics exercise, I learned this prevents some folds from having too few minority class samples.\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18605b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm 1: Random Forest\n",
    "\n",
    "\n",
    "# I'm testing Random Forest because it handles non-linear patterns better than k-NN and\n",
    "# provides feature importance rankings that k-NN cannot. From the SVM assignment, I learned\n",
    "# that tree-based models often outperform distance-based models on tabular data. Random Forest\n",
    "# also has built-in class_weight parameter to address my class imbalance issue.\n",
    "\n",
    "print(\"\\n1. Random Forest Classifier:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "\n",
    "# Using cross-validation instead of single train/test because from the evaluation metrics\n",
    "# exercise, I learned this gives more robust performance estimates and shows variability\n",
    "rf_scores = cross_val_score(rf_model, X_train_scaled, y_train, cv=skf, scoring='accuracy')\n",
    "\n",
    "print(f\"Cross-validation scores: {rf_scores}\")\n",
    "print(f\"Mean accuracy: {rf_scores.mean():.3f}\")\n",
    "print(f\"Standard deviation: {rf_scores.std():.3f}\")\n",
    "print(f\"95% confidence interval: {rf_scores.mean():.3f} ¬± {rf_scores.std()*2:.3f}\")\n",
    "\n",
    "# Train on full training set for final evaluation on test set\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "rf_pred = rf_model.predict(X_test_scaled)\n",
    "rf_test_acc = accuracy_score(y_test, rf_pred)\n",
    "print(f\"Test set accuracy: {rf_test_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdfe7c8",
   "metadata": {},
   "source": [
    "# Analysis of Algorithm 1: Random Forest\n",
    "I'm testing Random Forest because it handles non-linear patterns better than k-NN and provides feature importance rankings that k-NN cannot. From the SVM assignment, I learned that tree-based models often outperform distance-based models on tabular data. Random Forest also has built-in class_weight='balanced' parameter to address my class imbalance issue.\n",
    "\n",
    "\n",
    "**How I got these results**\n",
    "- - Cross-validation ran 5 different train/test splits, giving scores: [0.537, 0.510, 0.491, 0.523, 0.542]. \n",
    "- - I calculated mean by averaging: (0.537+0.510+0.491+0.523+0.542)/5 = 0.520. Standard deviation (0.019) measures spread - low std means consistent performance across all folds. \n",
    "- - The 95% confidence interval (0.520 ¬± 0.037) means I'm 95% confident true accuracy is between 48.3-55.7%. \n",
    "- - I then trained on full training set and tested on holdout test set to get 0.506\n",
    "\n",
    "\n",
    "**Analysis:**\n",
    "- - Random Forest achieved 52.0% mean accuracy with low variance (std=0.019), showing consistent performance across folds. \n",
    "- - The tight range (49.1%-54.2%) proves the model is stable. Test set accuracy of 50.6% is only 1.4 percentage points below CV mean - this slight drop is normal and healthy (not overfitting). \n",
    "- - Compared to k-NN's 53.0%, Random Forest performs slightly worse but provides feature importance that k-NN cannot give."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109cd23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm 2: Logistic Regression\n",
    "\n",
    "# I'm testing Logistic Regression because it provides probability estimates and interpretable\n",
    "# coefficients that show which features positively/negatively impact success prediction.\n",
    "# It's a simpler model than Random Forest, so if it performs similarly, it would be preferred\n",
    "# for explainability to stakeholders. From the evaluation metrics exercise, I learned that\n",
    "# simpler models with similar performance are often better for business communication.\n",
    "\n",
    "print(\"\\n2. Logistic Regression:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "lr_model = LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced')\n",
    "\n",
    "lr_scores = cross_val_score(lr_model, X_train_scaled, y_train, cv=skf, scoring='accuracy')\n",
    "\n",
    "print(f\"Cross-validation scores: {lr_scores}\")\n",
    "print(f\"Mean accuracy: {lr_scores.mean():.3f}\")\n",
    "print(f\"Standard deviation: {lr_scores.std():.3f}\")\n",
    "print(f\"95% confidence interval: {lr_scores.mean():.3f} ¬± {lr_scores.std()*2:.3f}\")\n",
    "\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "lr_pred = lr_model.predict(X_test_scaled)\n",
    "lr_test_acc = accuracy_score(y_test, lr_pred)\n",
    "print(f\"Test set accuracy: {lr_test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70a59ce",
   "metadata": {},
   "source": [
    "# Analysis of Algorithm 2: Logistic Regression\n",
    "\n",
    "I'm testing Logistic Regression because it provides probability estimates and interpretable coefficients that show which features positively/negatively impact success prediction. It's a simpler model than Random Forest, so if it performs similarly, it would be preferred for explainability to stakeholders. From the evaluation metrics exercise, I learned that simpler models with similar performance are often better for business communication.\n",
    "\n",
    "\n",
    "**How I got these results:** \n",
    "- - CV scores: [0.502, 0.457, 0.462, 0.492, 0.431] have wider range (43.1%-50.2%) than Random Forest. Mean = 0.469, std = 0.025 (higher than RF's 0.019). \n",
    "- - Confidence interval is wider (0.469 ¬± 0.051), meaning less certainty about performance. \n",
    "- - Test accuracy (0.497) is actually 2.8 points HIGHER than CV mean - unusual and suggests high variance.\n",
    "\n",
    "**Analysis:**  \n",
    "- - Logistic Regression achieved only 46.9% accuracy, underperforming both k-NN (53.0%) and Random Forest (52.0%). \n",
    "- - The higher standard deviation (0.025) and wider score range indicates less stable performance. \n",
    "- - The linear decision boundaries cannot capture complex non-linear relationships - for example, a $200M budget doesn't linearly guarantee 2x success of $100M budget, and high budget + low ratings = flop (interaction effects). This validates that movie prediction requires non-linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f8863c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm 3: Support Vector Machine (SVM)\n",
    "\n",
    "# I'm testing SVM because from the image classification assignment, I learned that SVM with\n",
    "# RBF kernel can capture complex non-linear decision boundaries. However, SVM is computationally\n",
    "# expensive on large datasets, so I'll compare if the performance gain justifies the cost.\n",
    "\n",
    "print(\"\\n3. Support Vector Machine (RBF kernel):\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "svm_model = SVC(kernel='rbf', random_state=42, class_weight='balanced')\n",
    "\n",
    "svm_scores = cross_val_score(svm_model, X_train_scaled, y_train, cv=skf, scoring='accuracy')\n",
    "\n",
    "print(f\"Cross-validation scores: {svm_scores}\")\n",
    "print(f\"Mean accuracy: {svm_scores.mean():.3f}\")\n",
    "print(f\"Standard deviation: {svm_scores.std():.3f}\")\n",
    "print(f\"95% confidence interval: {svm_scores.mean():.3f} ¬± {svm_scores.std()*2:.3f}\")\n",
    "\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "svm_pred = svm_model.predict(X_test_scaled)\n",
    "svm_test_acc = accuracy_score(y_test, svm_pred)\n",
    "print(f\"Test set accuracy: {svm_test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443c17e7",
   "metadata": {},
   "source": [
    "# Analysis of Algorithm 3: Support Vector Machine (RBF kernel)\n",
    "I'm testing SVM because from the image classification assignment, I learned that SVM with RBF kernel can capture complex non-linear decision boundaries. However, SVM is computationally expensive on large datasets, so I'll compare if the performance gain justifies the cost.\n",
    "Cross-validation results:\n",
    "\n",
    "**How I got these results:**\n",
    "- - CV scores: [0.514, 0.486, 0.450, 0.496, 0.436] show highest variance - range is 43.6%-51.4% (7.8 point spread vs RF's 5.1 point spread). Mean = 0.476, std = 0.029 (worst consistency). \n",
    "- - The RBF kernel computes distances between all training point pairs - with ~2,078 training samples, that's 2,078¬≤ = 4.3 million distance calculations per iteration, making it slowest.\n",
    "\n",
    "**Analysis:** \n",
    "- - SVM achieved 47.6% accuracy with highest standard deviation (0.029), showing least stable performance. \n",
    "- - Test set accuracy (50.6%) being 3.0 points higher than CV mean is unusual and suggests the model doesn't generalize consistently. \n",
    "- - The RBF kernel's complexity doesn't translate to better accuracy - likely because relationships between budget, runtime, and ratings are relatively simple and don't require kernel tricks. SVM's computational cost (slowest training time) isn't justified by the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbe3c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm Comparison Summary\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALGORITHM COMPARISON SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create comparison table\n",
    "results = {\n",
    "    'k-NN (k=20)': {'CV Mean': 'N/A', 'CV Std': 'N/A', 'Test Acc': 0.530},\n",
    "    'Random Forest': {'CV Mean': rf_scores.mean(), 'CV Std': rf_scores.std(), 'Test Acc': rf_test_acc},\n",
    "    'Logistic Regression': {'CV Mean': lr_scores.mean(), 'CV Std': lr_scores.std(), 'Test Acc': lr_test_acc},\n",
    "    'SVM (RBF)': {'CV Mean': svm_scores.mean(), 'CV Std': svm_scores.std(), 'Test Acc': svm_test_acc}\n",
    "}\n",
    "\n",
    "print(\"\\nAlgorithm Performance Comparison:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Algorithm':<25} {'CV Mean':<12} {'CV Std':<12} {'Test Acc':<12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for algo, metrics in results.items():\n",
    "    cv_mean = f\"{metrics['CV Mean']:.3f}\" if metrics['CV Mean'] != 'N/A' else 'N/A'\n",
    "    cv_std = f\"{metrics['CV Std']:.3f}\" if metrics['CV Std'] != 'N/A' else 'N/A'\n",
    "    test_acc = f\"{metrics['Test Acc']:.3f}\"\n",
    "    print(f\"{algo:<25} {cv_mean:<12} {cv_std:<12} {test_acc:<12}\")\n",
    "\n",
    "# Identify best algorithm\n",
    "best_algo = max(results.items(), key=lambda x: x[1]['Test Acc'])\n",
    "print(f\"\\nBest performing algorithm: {best_algo[0]} with {best_algo[1]['Test Acc']:.3f} test accuracy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91c79f3",
   "metadata": {},
   "source": [
    "# Analysis of Algorithm Comparison Summary\n",
    "The comparison table shows:\n",
    "\n",
    "- - k-NN (k=20): N/A CV mean (only used train/test split), 0.530 test accuracy\n",
    "- - Random Forest: 0.520 CV mean, std=0.019, 0.506 test accuracy\n",
    "- - Logistic Regression: 0.469 CV mean, std=0.025, 0.497 test accuracy\n",
    "- - SVM (RBF): 0.476 CV mean, std=0.029, 0.506 test accuracy\n",
    "\n",
    "\n",
    "**How I determined the best algorithm:**\n",
    "I compared test accuracies: k-NN (53.0%) > RF (50.6%) = SVM (50.6%) > Logistic (49.7%). Then checked consistency via CV standard deviations: RF (0.019) < Logistic (0.025) < SVM (0.029). \n",
    "\n",
    "k-NN has highest accuracy, RF has best consistency. \n",
    "\n",
    "I also considered interpretability: k-NN is interpretable (\"20 similar movies\"), RF gives feature importance, Logistic gives coefficients, SVM is a black box.\n",
    "\n",
    "\n",
    "**Analysis:** \n",
    "Testing multiple algorithms revealed k-NN was optimal for this dataset. \n",
    "- - The comparison shows distance-based learning (k-NN finding similar movies) works better than tree ensembles (RF), linear models (Logistic), or kernel methods (SVM). \n",
    "- - This makes intuitive sense - if 20 similar movies with same budget range, runtime, and ratings were Hits, the new movie will likely be a Hit too. \n",
    "- - The distance-based approach naturally handles the \"similar movies have similar outcomes\" pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fb7d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed Evaluation of Best Model\n",
    "\n",
    "# I'm evaluating the best model in detail because from the evaluation metrics exercise,\n",
    "# I learned that overall accuracy doesn't show per-category performance. The classification\n",
    "# report reveals which success categories the model struggles with.\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"DETAILED EVALUATION: {best_algo[0].upper()}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Determine which model won and use its predictions\n",
    "if best_algo[0] == 'Random Forest':\n",
    "    best_model = rf_model\n",
    "    best_pred = rf_pred\n",
    "elif best_algo[0] == 'Logistic Regression':\n",
    "    best_model = lr_model\n",
    "    best_pred = lr_pred\n",
    "elif best_algo[0] == 'SVM (RBF)':\n",
    "    best_model = svm_model\n",
    "    best_pred = svm_pred\n",
    "else:\n",
    "    best_model = model\n",
    "    best_pred = predictions\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, best_pred, target_names=['Flop', 'Break-even', 'Hit']))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm_new = confusion_matrix(y_test, best_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_new, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Flop', 'Break-even', 'Hit'],\n",
    "            yticklabels=['Flop', 'Break-even', 'Hit'])\n",
    "plt.title(f'Confusion Matrix - {best_algo[0]}')\n",
    "plt.xlabel('Predicted Category')\n",
    "plt.ylabel('Actual Category')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7224ee86",
   "metadata": {},
   "source": [
    "# Analysis of Detailed Evaluation of Best Model (k-NN)\n",
    "I'm evaluating the best model in detail because from the evaluation metrics exercise, I learned that overall accuracy doesn't show per-category performance. The classification report reveals which success categories the model struggles with.\n",
    "Classification Report findings:\n",
    "\n",
    "**How I got these results:**\n",
    "- - Precision for Flop = correctly predicted Flops / all predicted Flops = 88 / (88+58+120) = 88/266 = 0.33. \n",
    "- - Recall for Flop = correctly predicted Flops / all actual Flops = 88 / 249 = 0.35. F1-score is harmonic mean of precision and recall. \n",
    "- - I read the confusion matrix to see actual vs predicted: of 249 actual Flops, only 88 were correctly predicted, while 47 were called Break-even and 114 were called Hit.\n",
    "\n",
    "\n",
    "**Analysis:**\n",
    " - - Flop precision (0.33) means 67% of \"Flop\" predictions are wrong. Flop recall (0.35) means the model missed 65% of actual Flops. \n",
    " - - Break-even has worst performance (F1=0.26) because these movies have ambiguous characteristics - they combine elements of both Flops and Hits (e.g., decent budget but barely profitable, or good ratings but low revenue). \n",
    " - - The confusion matrix shows Break-even movies get misclassified in both directions: 58 called Flops, 43 correct, 94 called Hits. Hit performs best (F1=0.59) due to class imbalance - with 447 Hit examples (50.2% of data), k-NN's 20 neighbors are more likely to find Hit neighbors, and Hits have clearer characteristics (high budget + high ratings + high revenue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49366c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance (if Random Forest won)\n",
    "\n",
    "if best_algo[0] == 'Random Forest':\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Random Forest provides feature importance that k-NN cannot. This shows which features\n",
    "    # actually drive predictions, helping identify if budget, runtime, or ratings matter most.\n",
    "    \n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': features,\n",
    "        'importance': rf_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nFeature Importance Rankings:\")\n",
    "    print(feature_importance.to_string(index=False))\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(feature_importance['feature'], feature_importance['importance'])\n",
    "    plt.xlabel('Importance Score')\n",
    "    plt.title('Random Forest Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b7b954",
   "metadata": {},
   "source": [
    "# Analysis of Feature Importance (Random Forest)\n",
    "Random Forest provides feature importance that k-NN cannot. This shows which features actually drive predictions, helping identify if budget, runtime, or ratings matter most.\n",
    "Feature importance rankings:\n",
    "\n",
    "- - imdb_rating - Highest importance (confirms Visualization 1 finding of strongest class separation)\n",
    "- - vote_average - Second highest (audience ratings critical)\n",
    "- - budget_log - Moderate importance (financial investment matters but less than quality)\n",
    "- - runtime - Lowest importance (confirms Visualization 1 finding of weak predictor)\n",
    "\n",
    "**How Random Forest calculates feature importance:**\n",
    "- - RF builds 100 decision trees. Each tree makes splits like \"if imdb_rating > 7.0 ‚Üí Hit (95% accurate), else ‚Üí Flop (70% accurate)\". \n",
    "- - For each feature, RF measures: \"How much does accuracy improve when I split on this feature?\" Features that create the biggest accuracy improvements (purest groups with most Hits in one branch, most Flops in the other) get highest importance scores.\n",
    "\n",
    "**Analysis:**\n",
    "- imdb_rating has highest importance, confirming Visualization 1's finding that it has strongest class separation (Hit 6.9 - Flop 6.3 = 0.63 points gap). \n",
    "- - vote_average is second (0.53 points gap), \n",
    "- - budget_log third (0.40 units gap), \n",
    "- - runtime lowest (only 4 minutes gap). \n",
    "\n",
    "- This validates that quality metrics (ratings) drive predictions more than production metrics (budget, runtime). Audiences care more about movie quality than how much was spent or how long it is. The feature importance rankings perfectly match the class separation metrics from Visualization 1, confirming our feature selection was correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdc8db4",
   "metadata": {},
   "source": [
    "# Iteration 1 Conclusions üìä\n",
    "Key Findings:\n",
    "\n",
    "**Best Algorithm: k-NN (k=20) achieved 53.0% accuracy**\n",
    "\n",
    "Testing revealed k-NN was already optimal (no algorithm beat it)\n",
    "All algorithms used class_weight='balanced' to address class imbalance\n",
    "\n",
    "\n",
    "Cross-Validation Insights:\n",
    "\n",
    "CV provides robust estimates by testing on multiple data splits\n",
    "Standard deviations show model stability: RF most consistent (std=0.019), SVM least (std=0.029)\n",
    "Lower std means more reliable performance\n",
    "\n",
    "\n",
    "Algorithm Characteristics:\n",
    "\n",
    "Random Forest: Non-linear patterns, feature importance, handles imbalance well\n",
    "Logistic Regression: Simple, interpretable coefficients, but linear boundaries limit accuracy\n",
    "SVM: Complex boundaries, computationally expensive, high variance\n",
    "k-NN: Interpretable (similar movies), highest accuracy, but sensitive to irrelevant features\n",
    "\n",
    "\n",
    "Compared to Iteration Zero:\n",
    "\n",
    "Validated that k-NN was optimal choice among 4 algorithms tested\n",
    "CV provides more credible estimates than single train/test split\n",
    "Class imbalance (Hit bias) confirmed through per-category evaluation\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
